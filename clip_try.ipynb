{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <85A36C65-3F71-3C3B-B529-961AE17DBE73> /Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <F8622D92-25A9-3A61-A089-C917FDA36C1B> /Users/szaboreka/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # use CUDA device\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\") # use MacOS GPU device (e.g., for M2 chips)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # use CPU device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CLIP model - ViT B32\n",
    "model, preprocess = clip.load('ViT-B/32', device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define class names in a list - it needs prompt engineering\n",
    "class_names = [\"a photo of industrial plants with no smoke above chimney\", \"a photo of industrial plants emiting smoke from chimney\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create one long image from video frames\n",
    "def preprocess_video_to_image(video_path):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    #Handle if video can't be opened\n",
    "    if not video.isOpened():\n",
    "        print(\"Video file couldn't be opened\")\n",
    "    #If yes, read all video frames until the end of the video and append every frame to the frames list\n",
    "    else:\n",
    "        while True:\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        #Release video\n",
    "        video.release()\n",
    "    #Concetanate the frames in the list together\n",
    "    concatenated_frame = np.concatenate(frames, axis=1)\n",
    "    return concatenated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a square-shaped image from the video (similar to 1 long image)\n",
    "def preprocess_video_to_image_grid_version(video_path, num_rows=6, num_cols=6):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    if not video.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "    else:\n",
    "        while True:\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        video.release()\n",
    "    \n",
    "    # Create grids and store rows in the grids\n",
    "    grid = []\n",
    "    for i in range(num_rows):\n",
    "        row = np.concatenate(frames[i * num_cols: (i + 1) * num_cols], axis=1)\n",
    "        grid.append(row)\n",
    "    \n",
    "    # Concatenate grid vertically to create a single square-shaped image from the smoke video\n",
    "    concatenated_frame = np.concatenate(grid, axis=0)\n",
    "    return concatenated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON metadata\n",
    "with open('data/datasets/experimental_ijmond_dataset.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "ijmond_data = pd.DataFrame(data)\n",
    "\n",
    "#Define Torch Dataset class\n",
    "class ImageTitleDataset(Dataset):\n",
    "    def __init__(self, list_video_path, list_labels):\n",
    "\n",
    "        self.video_path = list_video_path\n",
    "        self.labels = list_labels\n",
    "        self.class_names = class_names\n",
    "        #self.transforms = ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess_video_to_image_grid_version(self.video_path[idx])\n",
    "        image = Image.fromarray(image)\n",
    "        image = preprocess(image)\n",
    "        label = self.labels[idx]\n",
    "        label = self.class_names[label]\n",
    "        label = clip.tokenize(label, context_length=77, truncate=True)\n",
    "        return image, label\n",
    "\n",
    "# Prepare the list of video file paths and labels\n",
    "list_video_path = [os.path.join(\"data/ijmond_videos/\", f\"{fn}.mp4\") for fn in ijmond_data['file_name']]\n",
    "#list_labels = dataset['label'].tolist()\n",
    "list_labels = [int(label) for label in ijmond_data['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(list_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and data loader\n",
    "dataset = ImageTitleDataset(list_video_path, list_labels)\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert model's parameters to FP32 format\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "\n",
    "if device == \"cpu\":\n",
    "  model.float()\n",
    "\n",
    "# Prepare the optimizer - weight from other user\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "\n",
    "# Specify the loss function\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, Loss: 1.9579:   0%|          | 0/7 [02:12<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[18.5469, 18.6719, 19.3438, 18.6250],\n",
      "        [19.2969, 19.0156, 19.5781, 19.4062],\n",
      "        [19.2969, 19.0156, 19.5781, 19.4062],\n",
      "        [18.5469, 18.6719, 19.3438, 18.6250]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[20.5156, 20.2031, 17.4062, 20.5156],\n",
      "        [19.0156, 18.6719, 16.5781, 19.0781],\n",
      "        [19.0156, 18.6719, 16.5781, 19.0781],\n",
      "        [19.0156, 18.6719, 16.5781, 19.0781]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[21.1875, 23.0312, 23.1094, 23.8750],\n",
      "        [20.2969, 22.1562, 22.1094, 22.9375],\n",
      "        [21.1875, 23.0312, 23.1094, 23.8750],\n",
      "        [21.1875, 23.0312, 23.1094, 23.8750]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[23.1094, 23.1250, 23.1406, 23.3594],\n",
      "        [23.1094, 23.1250, 23.1406, 23.3594],\n",
      "        [23.1094, 23.1250, 23.1406, 23.3594],\n",
      "        [23.3125, 23.3594, 23.3281, 23.5625]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[23.2812, 23.7812, 23.4375, 23.1875],\n",
      "        [23.2812, 23.7812, 23.4375, 23.1875],\n",
      "        [23.2812, 23.7812, 23.4375, 23.1875],\n",
      "        [23.0625, 23.5625, 23.2344, 23.0938]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[22.4844, 23.2031, 22.7344, 22.5469],\n",
      "        [22.4844, 23.2031, 22.7344, 22.5469],\n",
      "        [22.4844, 23.2031, 22.7344, 22.5469],\n",
      "        [22.7656, 23.4531, 23.0156, 22.8906]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([2, 77])\n",
      "tensor([[23.5000, 24.0156],\n",
      "        [23.5000, 24.0156]], dtype=torch.float16, grad_fn=<TBackward0>)\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, Loss: 0.7096:   0%|          | 0/7 [10:54<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[23.4688, 23.5938, 23.2812, 23.5625],\n",
      "        [23.4688, 23.5938, 23.2812, 23.5625],\n",
      "        [23.4688, 23.5938, 23.2812, 23.5625],\n",
      "        [23.3125, 23.5469, 23.1875, 23.5312]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.3781:   0%|          | 0/7 [01:40<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[24.1719, 23.8594, 23.0312, 23.6094],\n",
      "        [24.1719, 23.8594, 23.0312, 23.6094],\n",
      "        [23.9375, 23.6406, 22.9375, 23.5000],\n",
      "        [23.9375, 23.6406, 22.9375, 23.5000]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.3926:   0%|          | 0/7 [03:21<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[23.2188, 23.6094, 23.3281, 23.6094],\n",
      "        [23.2500, 23.7969, 23.4375, 23.6094],\n",
      "        [23.2500, 23.7969, 23.4375, 23.6094],\n",
      "        [23.2188, 23.6094, 23.3281, 23.6094]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.3626:   0%|          | 0/7 [05:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[23.5312, 23.6562, 23.2812, 23.6250],\n",
      "        [23.5000, 23.6875, 23.3906, 23.6094],\n",
      "        [23.5000, 23.6875, 23.3906, 23.6094],\n",
      "        [23.5000, 23.6875, 23.3906, 23.6094]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.3764:   0%|          | 0/7 [06:50<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[23.2500, 22.5312, 23.2812, 23.4844],\n",
      "        [23.2500, 22.5312, 23.2812, 23.4844],\n",
      "        [23.2500, 22.5312, 23.2812, 23.4844],\n",
      "        [23.1719, 22.6094, 23.1875, 23.5625]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.3945:   0%|          | 0/7 [08:35<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[22.9375, 22.6094, 23.6094, 23.1562],\n",
      "        [22.9844, 22.7344, 23.4688, 23.1562],\n",
      "        [22.9375, 22.6094, 23.6094, 23.1562],\n",
      "        [22.9375, 22.6094, 23.6094, 23.1562]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.3870:   0%|          | 0/7 [10:16<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([2, 77])\n",
      "tensor([[22.8125, 22.8594],\n",
      "        [22.7656, 22.8906]], dtype=torch.float16, grad_fn=<TBackward0>)\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6743:   0%|          | 0/7 [11:14<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[23.2344, 23.1406, 23.1562, 22.7656],\n",
      "        [22.8906, 23.0312, 23.0000, 22.4219],\n",
      "        [22.8906, 23.0312, 23.0000, 22.4219],\n",
      "        [22.8906, 23.0312, 23.0000, 22.4219]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[22.8125, 22.6875, 22.8125, 22.8125],\n",
      "        [22.8125, 22.6875, 22.8125, 22.8125],\n",
      "        [22.8125, 22.6875, 22.8125, 22.8125],\n",
      "        [22.8125, 22.6875, 22.8125, 22.8125]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[23.0156, 22.9219, 22.8906, 22.7969],\n",
      "        [23.0156, 22.9219, 22.8906, 22.7969],\n",
      "        [23.0156, 22.9219, 22.8906, 22.7969],\n",
      "        [23.0156, 22.9219, 22.8906, 22.7969]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[22.5625, 22.5625, 22.4219, 22.8125],\n",
      "        [22.5625, 22.5625, 22.4219, 22.8125],\n",
      "        [22.6406, 22.7656, 22.7969, 22.7344],\n",
      "        [22.5625, 22.5625, 22.4219, 22.8125]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[22.6562, 22.8125, 22.5000, 22.7031],\n",
      "        [22.6562, 22.8125, 22.5000, 22.7031],\n",
      "        [22.6562, 22.8125, 22.5000, 22.7031],\n",
      "        [22.6562, 22.8125, 22.5000, 22.7031]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor before forward pass torch.Size([4, 77])\n",
      "tensor([[22.5000, 22.6094, 23.0000, 22.7188],\n",
      "        [22.5000, 22.6094, 23.0000, 22.7188],\n",
      "        [22.5000, 22.6094, 23.0000, 22.7188],\n",
      "        [22.5000, 22.6094, 23.0000, 22.7188]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "  pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "  for batch in train_dataloader :\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      images,texts = batch \n",
    "    \n",
    "      images= images.to(device)\n",
    "      texts = texts.to(device)\n",
    "\n",
    "      # Debugging: Print the shape of the input tensor\n",
    "      texts = texts.squeeze(dim = 1)\n",
    "      print(\"Shape of input tensor before forward pass\", texts.shape)\n",
    "      #images = torch.stack([img for img in images],dim=0)\n",
    "\n",
    "      # Forward pass\n",
    "      logits_per_image, logits_per_text = model(images, texts)\n",
    "      print(logits_per_text)\n",
    "\n",
    "      # Compute loss\n",
    "      #ground_truth = torch.tensor(texts, dtype=torch.long, device=device)\n",
    "      ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "      print(ground_truth)\n",
    "\n",
    "      logits_per_image = logits_per_image.float()\n",
    "      logits_per_text = logits_per_text.float()\n",
    "\n",
    "      total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "      \n",
    "      # Backward pass\n",
    "      total_loss.backward()\n",
    "      if device == \"cpu\":\n",
    "         optimizer.step()\n",
    "      else : \n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "\n",
    "      pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length: 26\n",
      "Sample: 0\n",
      "Image Shape: torch.Size([3, 224, 224])\n",
      "Label: tensor([[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "         26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)\n",
      "Sample: 1\n",
      "Image Shape: torch.Size([3, 224, 224])\n",
      "Label: tensor([[49406,   320,  1125,   539,  7520,  5829,   593,   871,  6664,  4348,\n",
      "         26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)\n",
      "Sample: 2\n",
      "Image Shape: torch.Size([3, 224, 224])\n",
      "Label: tensor([[49406,   320,  1125,   539,  7520,  5829,   593,   871,  6664,  4348,\n",
      "         26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#Inspect a few examples in dataset\n",
    "\n",
    "# Create dataset\n",
    "dataset = ImageTitleDataset(list_video_path, list_labels)\n",
    "print(\"Dataset Length:\", len(dataset))\n",
    "\n",
    "# Inspect 3 samples\n",
    "for i in range(3):\n",
    "    image, label = dataset[i]\n",
    "    print(\"Sample:\", i)\n",
    "    print(\"Image Shape:\", image.shape)\n",
    "    print(\"Label:\", label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Images Shape: torch.Size([4, 3, 224, 224])\n",
      "Batch Labels: tensor([[[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   593,   871,  6664,  4348,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   593,   871,  6664,  4348,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]]],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#Inspect Batch sizes\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Iterate over a few batches\n",
    "for images, labels in dataloader:\n",
    "    print(\"Batch Images Shape:\", images.shape)\n",
    "    print(\"Batch Labels:\", labels)\n",
    "    break  # Stop after first batch\n",
    "\n",
    "# (batch_size, channel, time, height, width)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
