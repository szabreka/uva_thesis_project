{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "import numpy as np\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 427,616,513\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-L/14\")\n",
    "model.eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/16\")\n",
    "model.eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter positional_embedding, shape torch.Size([77, 512])\n",
      "Parameter text_projection, shape torch.Size([512, 512])\n",
      "Parameter logit_scale, shape torch.Size([])\n",
      "Parameter visual.class_embedding, shape torch.Size([768])\n",
      "Parameter visual.positional_embedding, shape torch.Size([197, 768])\n",
      "Parameter visual.proj, shape torch.Size([768, 512])\n",
      "Parameter visual.conv1.weight, shape torch.Size([768, 3, 16, 16])\n",
      "Parameter visual.ln_pre.weight, shape torch.Size([768])\n",
      "Parameter visual.ln_pre.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.0.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.0.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.0.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.0.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.0.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.0.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.0.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.0.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.0.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.0.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.0.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.0.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.1.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.1.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.1.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.1.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.1.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.1.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.1.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.1.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.1.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.1.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.1.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.1.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.2.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.2.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.2.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.2.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.2.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.2.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.2.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.2.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.2.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.2.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.2.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.2.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.3.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.3.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.3.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.3.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.3.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.3.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.3.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.3.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.3.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.3.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.3.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.3.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.4.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.4.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.4.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.4.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.4.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.4.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.4.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.4.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.4.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.4.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.4.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.4.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.5.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.5.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.5.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.5.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.5.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.5.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.5.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.5.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.5.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.5.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.5.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.5.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.6.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.6.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.6.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.6.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.6.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.6.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.6.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.6.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.6.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.6.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.6.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.6.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.7.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.7.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.7.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.7.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.7.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.7.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.7.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.7.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.7.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.7.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.7.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.7.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.8.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.8.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.8.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.8.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.8.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.8.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.8.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.8.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.8.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.8.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.8.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.8.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.9.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.9.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.9.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.9.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.9.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.9.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.9.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.9.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.9.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.9.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.9.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.9.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.10.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.10.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.10.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.10.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.10.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.10.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.10.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.10.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.10.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.10.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.10.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.10.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.11.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.11.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.11.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.11.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.11.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.11.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.11.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.11.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.11.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.11.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.11.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.11.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.ln_post.weight, shape torch.Size([768])\n",
      "Parameter visual.ln_post.bias, shape torch.Size([768])\n",
      "Parameter transformer.resblocks.0.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.0.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.0.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.0.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.0.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.0.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.0.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.0.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.0.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.0.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.0.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.0.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.1.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.1.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.1.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.1.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.1.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.1.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.1.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.1.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.1.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.1.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.1.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.1.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.2.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.2.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.2.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.2.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.2.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.2.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.2.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.2.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.2.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.2.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.2.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.2.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.3.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.3.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.3.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.3.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.3.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.3.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.3.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.3.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.3.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.3.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.3.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.3.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.4.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.4.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.4.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.4.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.4.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.4.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.4.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.4.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.4.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.4.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.4.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.4.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.5.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.5.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.5.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.5.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.5.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.5.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.5.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.5.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.5.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.5.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.5.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.5.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.6.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.6.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.6.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.6.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.6.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.6.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.6.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.6.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.6.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.6.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.6.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.6.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.7.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.7.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.7.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.7.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.7.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.7.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.7.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.7.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.7.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.7.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.7.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.7.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.8.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.8.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.8.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.8.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.8.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.8.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.8.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.8.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.8.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.8.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.8.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.8.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.9.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.9.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.9.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.9.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.9.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.9.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.9.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.9.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.9.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.9.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.9.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.9.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.10.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.10.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.10.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.10.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.10.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.10.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.10.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.10.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.10.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.10.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.10.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.10.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.11.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.11.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.11.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.11.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.11.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.11.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.11.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.11.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.11.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.11.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.11.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.11.ln_2.bias, shape torch.Size([512])\n",
      "Parameter token_embedding.weight, shape torch.Size([49408, 512])\n",
      "Parameter ln_final.weight, shape torch.Size([512])\n",
      "Parameter ln_final.bias, shape torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter {name}, shape {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=warn)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x17764ee80>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
