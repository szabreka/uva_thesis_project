{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP as binary image classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # use CUDA device\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\") # use MacOS GPU device (e.g., for M2 chips)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # use CPU device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CLIP model - ViT B32\n",
    "model, preprocess = clip.load('ViT-B/16', device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "class ImageTitleDataset(Dataset):\n",
    "    def __init__(self, list_video_path, list_labels, class_names, transform_image):\n",
    "        #to handle the parent class\n",
    "        super().__init__()\n",
    "        #Initalize image paths and corresponding texts\n",
    "        self.video_path = list_video_path\n",
    "        #Initialize labels (0 or 1)\n",
    "        self.labels = list_labels\n",
    "        #Initialize class names (no smoke or smoke)\n",
    "        self.class_names = class_names\n",
    "        #Transform to tensor\n",
    "        #self.transforms = ToTensor()\n",
    "        self.transform_image = transform_image\n",
    "\n",
    "    @staticmethod\n",
    "    #Function to create a square-shaped image from the video (similar to 1 long image)\n",
    "    #To do: what if the video has more frames than 36?\n",
    "    def preprocess_video_to_image_grid_version(video_path, num_rows=6, num_cols=6):\n",
    "        #Open the video file\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        #Create list for extracted frames\n",
    "        frames = []\n",
    "        #Handle if video can't be opened\n",
    "        if not video.isOpened():\n",
    "            print(\"Error: Could not open video file\")\n",
    "        else:\n",
    "            while True:\n",
    "                is_read, frame = video.read()\n",
    "                if not is_read:\n",
    "                    break\n",
    "                frames.append(frame)\n",
    "            video.release()\n",
    "        \n",
    "        if len(frames) != 36:\n",
    "            print(\"Num of frames are not 36\")\n",
    "            print(\"Num of frames for video on \", video_path, \"is \", len(frames))\n",
    "        \n",
    "        # Create  and store rows in the grids\n",
    "        rows_list = []\n",
    "        for i in range(num_rows):\n",
    "            #create rows from the frames using indexes -- for example, if i=0, then between the 0th and 6th frame\n",
    "            row = np.concatenate(frames[i * num_cols: (i + 1) * num_cols], axis=1)\n",
    "            rows_list.append(row)\n",
    "        \n",
    "        # Concatenate grid vertically to create a single square-shaped image from the smoke video\n",
    "        concatenated_frames = np.concatenate(rows_list, axis=0)\n",
    "        return concatenated_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #tranform videos into images and preprocess with clip's function\n",
    "        video_path = self.video_path[idx]\n",
    "        image = self.preprocess_video_to_image_grid_version(video_path)\n",
    "        image = Image.fromarray(image)\n",
    "        image = self.transform_image(image)\n",
    "        #image = preprocess(image)\n",
    "        #get the corresponding class names and tokenize\n",
    "        true_label = self.labels[idx]\n",
    "        label = self.class_names[true_label]\n",
    "        label = clip.tokenize(label, context_length=77, truncate=True)\n",
    "        return image, label, true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training data\n",
    "# Load the JSON metadata\n",
    "with open('data/datasets/experimental_ijmond_dataset.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "train_data = pd.DataFrame(train_data)\n",
    "# Prepare the list of video file paths and labels\n",
    "list_video_path = [os.path.join(\"data/ijmond_videos/\", f\"{fn}.mp4\") for fn in train_data['file_name']]\n",
    "#list_labels = dataset['label'].tolist()\n",
    "list_labels = [int(label) for label in train_data['label']]\n",
    "#Define class names in a list - it needs prompt engineering\n",
    "class_names = [\"a photo of factories with clear sky above chimney\", \"a photo of factories emiting smoke from chimney\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input resolution\n",
    "input_resolution = (224, 224)\n",
    "\n",
    "# Define the transformation pipeline - from CLIP preprocessor without random crop augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(input_resolution, interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and data loader for training\n",
    "dataset = ImageTitleDataset(list_video_path, list_labels, class_names, transform)\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and data loader for training\n",
    "dataset = ImageTitleDataset(list_video_path[20:], list_labels[20:], class_names, transform)\n",
    "val_data = DataLoader(dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define validation data\n",
    "# Load the JSON metadata\n",
    "with open('data/datasets/experimental_ijmond_dataset.json', 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "val_data = pd.DataFrame(val_data)\n",
    "# Prepare the list of video file paths and labels\n",
    "list_val_video_path = [os.path.join(\"data/ijmond_videos/\", f\"{fn}.mp4\") for fn in val_data['file_name']]\n",
    "#list_labels = dataset['label'].tolist()\n",
    "list_val_labels = [int(label) for label in val_data['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert model's parameters to FP32 format\n",
    "#This is done so that our model loads in the provided memory\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "# Check if the device is set to CPU\n",
    "if device == \"cpu\":\n",
    "  model.float()\n",
    "\n",
    "# Prepare the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "\n",
    "#The lr, betas, eps and weight decay are from the CLIP paper\n",
    "\n",
    "# Specify the loss functions - for images and for texts\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]/var/folders/9v/r3fdnxqn6v740_k7q9q14pym0000gn/T/ipykernel_99459/3105613334.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits_per_text shape after forward passing:  torch.Size([2, 4])\n",
      "Logits_per_image shape after forward passing:  torch.Size([4, 2])\n",
      "Logits_per_text after forward passing:  tensor([[21.1719, 20.7656],\n",
      "        [21.1562, 20.7500],\n",
      "        [21.1094, 20.7031],\n",
      "        [21.1094, 20.7031]], grad_fn=<PermuteBackward0>)\n",
      "Logits_per_image after forward passing:  tensor([[21.1719, 20.7656],\n",
      "        [21.1562, 20.7500],\n",
      "        [21.1094, 20.7031],\n",
      "        [21.1094, 20.7031]], grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, Loss: 0.6121:  14%|█▍        | 1/7 [04:41<28:09, 281.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits_per_text shape after forward passing:  torch.Size([2, 4])\n",
      "Logits_per_image shape after forward passing:  torch.Size([4, 2])\n",
      "Logits_per_text after forward passing:  tensor([[21.2812, 20.6875],\n",
      "        [21.2812, 20.6875],\n",
      "        [21.2969, 20.7031],\n",
      "        [21.2969, 20.7031]], grad_fn=<PermuteBackward0>)\n",
      "Logits_per_image after forward passing:  tensor([[21.2812, 20.6875],\n",
      "        [21.2812, 20.6875],\n",
      "        [21.2969, 20.7031],\n",
      "        [21.2969, 20.7031]], grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, Loss: 0.7366:  29%|██▊       | 2/7 [09:31<23:51, 286.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits_per_text shape after forward passing:  torch.Size([2, 4])\n",
      "Logits_per_image shape after forward passing:  torch.Size([4, 2])\n",
      "Logits_per_text after forward passing:  tensor([[21.3281, 20.9219],\n",
      "        [21.5312, 21.1250],\n",
      "        [21.5781, 21.1719],\n",
      "        [21.5781, 21.1719]], grad_fn=<PermuteBackward0>)\n",
      "Logits_per_image after forward passing:  tensor([[21.3281, 20.9219],\n",
      "        [21.5312, 21.1250],\n",
      "        [21.5781, 21.1719],\n",
      "        [21.5781, 21.1719]], grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, Loss: 0.7136:  43%|████▎     | 3/7 [14:28<19:24, 291.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits_per_text shape after forward passing:  torch.Size([2, 4])\n",
      "Logits_per_image shape after forward passing:  torch.Size([4, 2])\n",
      "Logits_per_text after forward passing:  tensor([[21.8438, 21.6406],\n",
      "        [21.7188, 21.5000],\n",
      "        [21.8906, 21.6719],\n",
      "        [21.9062, 21.7031]], grad_fn=<PermuteBackward0>)\n",
      "Logits_per_image after forward passing:  tensor([[21.8438, 21.6406],\n",
      "        [21.7188, 21.5000],\n",
      "        [21.8906, 21.6719],\n",
      "        [21.9062, 21.7031]], grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, Loss: 0.7495:  57%|█████▋    | 4/7 [19:14<14:28, 289.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits_per_text shape after forward passing:  torch.Size([2, 4])\n",
      "Logits_per_image shape after forward passing:  torch.Size([4, 2])\n",
      "Logits_per_text after forward passing:  tensor([[22.7031, 22.6562],\n",
      "        [22.6406, 22.5938],\n",
      "        [22.7031, 22.6562],\n",
      "        [22.6875, 22.6406]], grad_fn=<PermuteBackward0>)\n",
      "Logits_per_image after forward passing:  tensor([[22.7031, 22.6562],\n",
      "        [22.6406, 22.5938],\n",
      "        [22.7031, 22.6562],\n",
      "        [22.6875, 22.6406]], grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, Loss: 0.6934:  71%|███████▏  | 5/7 [23:42<09:23, 281.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits_per_text shape after forward passing:  torch.Size([2, 4])\n",
      "Logits_per_image shape after forward passing:  torch.Size([4, 2])\n",
      "Logits_per_text after forward passing:  tensor([[23.3281, 23.3438],\n",
      "        [23.2969, 23.3125],\n",
      "        [23.1875, 23.2188],\n",
      "        [23.2188, 23.2344]], grad_fn=<PermuteBackward0>)\n",
      "Logits_per_image after forward passing:  tensor([[23.3281, 23.3438],\n",
      "        [23.2969, 23.3125],\n",
      "        [23.1875, 23.2188],\n",
      "        [23.2188, 23.2344]], grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, Loss: 0.6873:  86%|████████▌ | 6/7 [28:11<04:37, 277.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits_per_text shape after forward passing:  torch.Size([2, 2])\n",
      "Logits_per_image shape after forward passing:  torch.Size([2, 2])\n",
      "Logits_per_text after forward passing:  tensor([[23.8281, 24.2344],\n",
      "        [23.7969, 24.2188]], grad_fn=<PermuteBackward0>)\n",
      "Logits_per_image after forward passing:  tensor([[23.8281, 24.2344],\n",
      "        [23.7969, 24.2188]], grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, Loss: 0.9215: 100%|██████████| 7/7 [30:39<00:00, 262.77s/it]\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(val_data, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_data)):\n\u001b[1;32m     65\u001b[0m           \u001b[38;5;66;03m# Extract images and texts from the batch\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m           images, labels, true_label \u001b[38;5;241m=\u001b[39m batch \n\u001b[1;32m     67\u001b[0m           \u001b[38;5;66;03m# Move images and texts to the specified device\u001b[39;00m\n\u001b[1;32m     68\u001b[0m           images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "  for batch in pbar:\n",
    "      # Extract images and texts from the batch\n",
    "      images, labels, true_label = batch \n",
    "\n",
    "      # Move images and texts to the specified device (CPU or GPU)\n",
    "      images= images.to(device)\n",
    "      texts = labels.to(device)\n",
    "      true_label = true_label.to(device)\n",
    "      text_inputs = clip.tokenize(class_names).to(device)\n",
    "\n",
    "      #Squeeze texts tensor to match the required size\n",
    "      texts = texts.squeeze(dim = 1)\n",
    "      text_inputs.squeeze(dim = 1)\n",
    "\n",
    "      # Forward pass - Run the model on the input data (images and texts)\n",
    "      #logits_per_image, logits_per_text = model(images, texts)\n",
    "      logits_per_image, logits_per_text = model(images, text_inputs)\n",
    "\n",
    "      #Get ground truth - here the true label\n",
    "      ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n",
    "\n",
    "      #Transform logits to float to match required dtype \n",
    "      logits_per_image = logits_per_image.float()\n",
    "      logits_per_text = logits_per_text.float()\n",
    "\n",
    "      #Inspect logits\n",
    "      print('Logits_per_text shape after forward passing: ', logits_per_text.shape)\n",
    "      print('Logits_per_image shape after forward passing: ', logits_per_image.shape)\n",
    "      print('Logits_per_text after forward passing: ', logits_per_text.T)\n",
    "      print('Logits_per_image after forward passing: ', logits_per_image)\n",
    "\n",
    "      #Compute loss - contrastive loss to pull similar pairs closer together\n",
    "      total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text.T,ground_truth))/2\n",
    "\n",
    "      #One image should match 1 label, but 1 label can match will multiple images (when single label classification)\n",
    "      #total_loss = loss_img(logits_per_image,ground_truth) \n",
    "      \n",
    "      # Zero out gradients for the optimizer (Adam) - to prevent adding gradients to previous ones\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Backward pass\n",
    "      total_loss.backward()\n",
    "      if device == \"cpu\":\n",
    "         optimizer.step()\n",
    "      else : \n",
    "        # Convert model's parameters to FP32 format, update, and convert back\n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "      # Update the progress bar with the current epoch and loss\n",
    "      pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "  model.eval()\n",
    "  val_losses = []\n",
    "  val_accs = []\n",
    "  all_preds = []\n",
    "  all_labels = []\n",
    "  with torch.no_grad():\n",
    "      for batch in tqdm(val_data, total=len(val_data)):\n",
    "            # Extract images and texts from the batch\n",
    "            images, labels, true_label = batch \n",
    "            # Move images and texts to the specified device\n",
    "            images = images.to(device)\n",
    "            texts = labels.to(device)\n",
    "            true_label = true_label.to(device)\n",
    "            text_inputs = clip.tokenize(class_names).to(device)\n",
    "            texts = texts.squeeze(dim=1)\n",
    "            text_inputs.squeeze(dim=1)\n",
    "\n",
    "            # Forward pass\n",
    "            logits_per_image, logits_per_text = model(images, text_inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n",
    "            logits_per_image = logits_per_image.float()\n",
    "            logits_per_text = logits_per_text.float()\n",
    "            total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text.T, ground_truth)) / 2\n",
    "\n",
    "            # Compute predicted labels\n",
    "            pred_labels = logits_per_image.argmax(dim=1)\n",
    "\n",
    "            print('Ground truth: ', ground_truth)\n",
    "            print('Perdicted label: ', pred_labels)\n",
    "\n",
    "            # Append predicted labels and ground truth labels\n",
    "            all_preds.extend(pred_labels.cpu().numpy())\n",
    "            all_labels.extend(ground_truth.cpu().numpy())\n",
    "\n",
    "            # Append loss\n",
    "            val_losses.append(total_loss.item())\n",
    "  \n",
    "  # Calculate confusion matrix\n",
    "  conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "  # Print or visualize the confusion matrix\n",
    "  print(\"Confusion Matrix:\")\n",
    "  print(conf_matrix)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine variables and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photo of industrial plants emiting smoke from chimney\n",
      "[1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]\n",
      "a photo of industrial plants emiting smoke from chimney\n"
     ]
    }
   ],
   "source": [
    "#try labels and class names\n",
    "print(class_names[1])\n",
    "print(list_labels)\n",
    "print(class_names[list_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of video reached during preprocessing\n",
      "tensor([[0.8315, 0.1685]])\n",
      "\n",
      "Predictions for frame 1:\n",
      "\n",
      "a photo of industrial plants with clear sky above chimney: 83.15%\n",
      "a photo of industrial plants emiting smoke from chimney: 16.85%\n",
      "tensor([[0.7511, 0.2489]])\n",
      "\n",
      "Predictions for frame 2:\n",
      "\n",
      "a photo of industrial plants with clear sky above chimney: 75.11%\n",
      "a photo of industrial plants emiting smoke from chimney: 24.89%\n",
      "tensor([[0.5893, 0.4107]])\n",
      "\n",
      "Predictions for frame 3:\n",
      "\n",
      "a photo of industrial plants with clear sky above chimney: 58.93%\n",
      "a photo of industrial plants emiting smoke from chimney: 41.07%\n",
      "tensor([[0.6620, 0.3380]])\n",
      "\n",
      "Predictions for frame 4:\n",
      "\n",
      "a photo of industrial plants with clear sky above chimney: 66.20%\n",
      "a photo of industrial plants emiting smoke from chimney: 33.80%\n",
      "tensor([[0.5639, 0.4361]])\n",
      "\n",
      "Predictions for frame 5:\n",
      "\n",
      "a photo of industrial plants with clear sky above chimney: 56.39%\n",
      "a photo of industrial plants emiting smoke from chimney: 43.61%\n",
      "tensor([[0.7712, 0.2288]])\n",
      "\n",
      "Predictions for frame 6:\n",
      "\n",
      "a photo of industrial plants with clear sky above chimney: 77.12%\n",
      "a photo of industrial plants emiting smoke from chimney: 22.88%\n",
      "tensor([[0.8326, 0.1674]])\n",
      "\n",
      "Predictions for frame 7:\n",
      "\n",
      "a photo of industrial plants with clear sky above chimney: 83.26%\n",
      "a photo of industrial plants emiting smoke from chimney: 16.74%\n",
      "tensor([[0.7646, 0.2354]])\n",
      "\n",
      "Predictions for frame 8:\n",
      "\n",
      "a photo of industrial plants with clear sky above chimney: 76.46%\n",
      "a photo of industrial plants emiting smoke from chimney: 23.54%\n",
      "tensor([[0.6749, 0.3251]])\n",
      "\n",
      "Predictions for frame 9:\n",
      "\n",
      "a photo of industrial plants with clear sky above chimney: 67.49%\n",
      "a photo of industrial plants emiting smoke from chimney: 32.51%\n",
      "tensor([[0.6437, 0.3563]])\n",
      "\n",
      "Predictions for frame 10:\n",
      "\n",
      "a photo of industrial plants with clear sky above chimney: 64.37%\n",
      "a photo of industrial plants emiting smoke from chimney: 35.63%\n",
      "tensor([[0.5469, 0.4531]])\n",
      "\n",
      "Predictions for frame 11:\n",
      "\n",
      "a photo of industrial plants with clear sky above chimney: 54.69%\n",
      "a photo of industrial plants emiting smoke from chimney: 45.31%\n",
      "tensor([[0.6014, 0.3986]])\n",
      "\n",
      "Predictions for frame 12:\n",
      "\n",
      "a photo of industrial plants with clear sky above chimney: 60.14%\n",
      "a photo of industrial plants emiting smoke from chimney: 39.86%\n",
      "tensor([[0.2651, 0.7349]])\n",
      "\n",
      "Predictions for frame 13:\n",
      "\n",
      "a photo of industrial plants emiting smoke from chimney: 73.49%\n",
      "a photo of industrial plants with clear sky above chimney: 26.51%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names[index]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>16s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mvalue\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m         i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 68\u001b[0m vanilla_clip(example_path)\n",
      "Cell \u001b[0;32mIn[3], line 51\u001b[0m, in \u001b[0;36mvanilla_clip\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Calculate features\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 51\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(image)\n\u001b[1;32m     52\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_text(text_inputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Calculate similarity\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/clip/model.py:341\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual(image\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/clip/model.py:232\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    229\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_pre(x)\n\u001b[1;32m    231\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n\u001b[1;32m    233\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    235\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_post(x[:, \u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/clip/model.py:203\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresblocks(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/clip/model.py:191\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    190\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x))\n\u001b[0;32m--> 191\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/16', device)\n",
    "example_path = 'data/ijmond_videos/5PurGkmy0aw-1.mp4'\n",
    "\n",
    "#define class names in a list - it need prompt engineering\n",
    "class_names = [\"a photo of industrial plants with clear sky above chimney\", \"a photo of industrial plants emiting smoke from chimney\"]\n",
    "\n",
    "#Crete a list of images from video\n",
    "def preprocess_video(video_path):\n",
    "    # Open the video file\n",
    "    # example video : video_path = 'data/ijmond_videos/5PurGkmy0aw-1.mp4'\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    if not video.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "    else:\n",
    "        i = 1\n",
    "        while True:\n",
    "            ret, image = video.read()\n",
    "            if ret == False:\n",
    "                print('End of video reached during preprocessing')\n",
    "                break\n",
    "            frames.append(image)\n",
    "            i += 1\n",
    "        video.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def vanilla_clip(video_path):\n",
    "    #Create image list from video\n",
    "    frames = preprocess_video(video_path)\n",
    "\n",
    "    # Loop over each frame in video (36 frames in 1 video)\n",
    "    i = 1\n",
    "    for frame in frames:\n",
    "        # Read image and preprocess\n",
    "        image = preprocess(Image.fromarray(frame)).unsqueeze(0).to(device)\n",
    "\n",
    "        # Prepare text inputs based on class names list\n",
    "        text_inputs = clip.tokenize(class_names).to(device)\n",
    "\n",
    "        # Calculate features\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "\n",
    "        # Calculate similarity\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        #values are the probabilities, indicies are the classes\n",
    "        print(similarity)\n",
    "        values, indices = similarity[0].topk(2)\n",
    "\n",
    "        # Print predictions for each frame\n",
    "        print(f\"\\nPredictions for frame {i}:\\n\")\n",
    "        for value, index in zip(values, indices):\n",
    "            print(f\"{class_names[index]:>16s}: {100 * value.item():.2f}%\")\n",
    "        i+=1\n",
    "\n",
    "vanilla_clip(example_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some techniques for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Store some constant - code to define params\n",
    "'''num_epochs = int(training_args.num_train_epochs)\n",
    "    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count() * training_args.gradient_accumulation_steps\n",
    "    eval_batch_size = int(training_args.per_device_eval_batch_size) * jax.device_count()\n",
    "    steps_per_epoch = len(train_dataset) // train_batch_size\n",
    "    total_train_steps = steps_per_epoch * num_epochs\n",
    "\n",
    "# Create learning rate schedule\n",
    "    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n",
    "        len(train_dataset),\n",
    "        train_batch_size,\n",
    "        training_args.num_train_epochs,\n",
    "        training_args.warmup_steps,\n",
    "        training_args.learning_rate,\n",
    "    )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of video reached during preprocessing\n",
      "A9W8G55JucU-3\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "True label: 1\n",
      "End of video reached during preprocessing\n",
      "tT4vETXW7Og-2\n",
      "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 1\n",
      "End of video reached during preprocessing\n",
      "aINMnqmwSUg-1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 1\n",
      "End of video reached during preprocessing\n",
      "cxfcZFPpflE-0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "True label: 0\n",
      "End of video reached during preprocessing\n",
      "Qv9-nS5BloI-2\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "True label: 1\n",
      "End of video reached during preprocessing\n",
      "9J-4qvCueZw-1\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "True label: 0\n",
      "End of video reached during preprocessing\n",
      "9J-4qvCueZw-2\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "True label: 1\n",
      "End of video reached during preprocessing\n",
      "rsBRGyFrPwM-1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 1\n",
      "End of video reached during preprocessing\n",
      "dmVLD2mvOAg-0\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 1\n",
      "End of video reached during preprocessing\n",
      "rsBRGyFrPwM-3\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 0\n",
      "End of video reached during preprocessing\n",
      "M7arDQZyAG4-3\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 1\n",
      "End of video reached during preprocessing\n",
      "HgNO44L2INY-1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 0\n",
      "End of video reached during preprocessing\n",
      "M7arDQZyAG4-0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 0\n",
      "End of video reached during preprocessing\n",
      "cIdp92dGCUo-2\n",
      "[0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "True label: 1\n",
      "End of video reached during preprocessing\n",
      "23x-vGYMbec-2\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 0\n",
      "End of video reached during preprocessing\n",
      "glLcsYu7mbM-3\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "True label: 1\n",
      "End of video reached during preprocessing\n",
      "inNVmNqCfDM-3\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 1\n",
      "End of video reached during preprocessing\n",
      "hSYtB254UZg-3\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 1\n",
      "End of video reached during preprocessing\n",
      "glLcsYu7mbM-0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "True label: 0\n",
      "End of video reached during preprocessing\n",
      "mRjzTfzMS1I-4\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "True label: 0\n",
      "End of video reached during preprocessing\n",
      "aFhpESIPnXg-4\n",
      "[1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1]\n",
      "True label: 0\n",
      "End of video reached during preprocessing\n",
      "vQZz9ePv_vQ-3\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 1\n",
      "End of video reached during preprocessing\n",
      "p5klKMpdREI-0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "True label: 0\n",
      "End of video reached during preprocessing\n",
      "Cjs_IIDUDVM-2\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 0\n",
      "End of video reached during preprocessing\n",
      "5PurGkmy0aw-1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "True label: 0\n",
      "End of video reached during preprocessing\n",
      "vQZz9ePv_vQ-1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True label: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from EDA.eda_functions import get_label\n",
    "\n",
    "#This model is able to predict the label of a video based on the frames + get the true label of the video\n",
    "\n",
    "# Define device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # use CUDA device\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") # use MacOS GPU device (e.g., for M1 chips)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # use CPU device\n",
    "\n",
    "#load model and ijmond dataset\n",
    "model, preprocess = clip.load('ViT-B/16', device)\n",
    "ijmond_df = pd.read_json('data/datasets/ijmond_dataset.json')\n",
    "\n",
    "#define class names in a list - it need prompt engineering\n",
    "class_names = [\"a photo of industrial plants with clear sky above chimney\", \"a photo of industrial plants emiting smoke from chimney\"]\n",
    "\n",
    "#Func to create a list of images from video\n",
    "def preprocess_video(video_path):\n",
    "    # Open the video file\n",
    "    # example video : video_path = 'data/ijmond_videos/5PurGkmy0aw-1.mp4'\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    if not video.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "    else:\n",
    "        i = 1\n",
    "        while True:\n",
    "            ret, image = video.read()\n",
    "            if ret == False:\n",
    "                print('End of video reached during preprocessing')\n",
    "                break\n",
    "            frames.append(image)\n",
    "            i += 1\n",
    "        video.release()\n",
    "    return frames\n",
    "\n",
    "#func to get the true label of the video\n",
    "def get_true_label(file_name):\n",
    "    row = ijmond_df[ijmond_df['file_name'] == file_name].iloc[0]\n",
    "    return get_label(row)\n",
    "\n",
    "#clip model to predict class for each frame in video\n",
    "def vanilla_clip(video_path, file_name):\n",
    "    #Create image list from video\n",
    "    frames = preprocess_video(video_path)\n",
    "\n",
    "    # Loop over each frame in video (36 frames in 1 video)\n",
    "    i = 1\n",
    "    prediction_list= []\n",
    "    for frame in frames:\n",
    "        # Read image and preprocess\n",
    "        image = preprocess(Image.fromarray(frame)).unsqueeze(0).to(device)\n",
    "\n",
    "        # Prepare text inputs based on class names list\n",
    "        text_inputs = clip.tokenize(class_names).to(device)\n",
    "\n",
    "        # Calculate features\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "\n",
    "        # Calculate similarity\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        values, indices = similarity[0].topk(1)\n",
    "        if class_names[indices] == \"a photo of industrial plants with clear sky above chimney\":\n",
    "            prediction_list.append(0)\n",
    "        else:\n",
    "            prediction_list.append(1)       \n",
    "\n",
    "        # Print predictions for each frame\n",
    "        #print(f\"\\nPredictions for frame {i}:\\n\")\n",
    "        #for value, index in zip(values, indices):\n",
    "        #    print(f\"{class_names[index]:>16s}: {100 * value.item():.2f}%\")\n",
    "        i+=1\n",
    "    print(file_name)\n",
    "    print(prediction_list)\n",
    "    print(\"True label:\", get_true_label(file_name))\n",
    "    if sum(prediction_list) >= 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#predict label for each video in ijmond dir\n",
    "files = os.listdir(\"data/ijmond_videos/\")\n",
    "for file in files:\n",
    "    video_path = f\"data/ijmond_videos/{file}\"\n",
    "    file_name = file.split('.')[0]\n",
    "    vanilla_clip(video_path, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used device:  cpu\n",
      "Datasets created\n",
      "Dataloaders created\n",
      "starts training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/var/folders/9v/r3fdnxqn6v740_k7q9q14pym0000gn/T/ipykernel_77584/4062299613.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:  tensor([[0.2258, 0.7742],\n",
      "        [0.3459, 0.6541],\n",
      "        [0.4480, 0.5520],\n",
      "        [0.0321, 0.9679],\n",
      "        [0.0504, 0.9496],\n",
      "        [0.5577, 0.4423],\n",
      "        [0.5708, 0.4292],\n",
      "        [0.0846, 0.9154]], grad_fn=<SoftmaxBackward0>)\n",
      "Index:  tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1]])\n",
      "Predicted label:  [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "Ground truth:  tensor([0, 1, 1, 0, 0, 1, 1, 0])\n",
      "Train accuracy:  0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/50, Loss: 1.6330:  25%|██▌       | 1/4 [00:06<00:18,  6.22s/it]/var/folders/9v/r3fdnxqn6v740_k7q9q14pym0000gn/T/ipykernel_77584/4062299613.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:  tensor([[0.4971, 0.5029],\n",
      "        [0.5078, 0.4922],\n",
      "        [0.5117, 0.4883],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5078, 0.4922],\n",
      "        [0.4961, 0.5039],\n",
      "        [0.5020, 0.4980],\n",
      "        [0.5078, 0.4922]], grad_fn=<SoftmaxBackward0>)\n",
      "Index:  tensor([[1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]])\n",
      "Predicted label:  [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Ground truth:  tensor([1, 0, 1, 0, 1, 1, 1, 0])\n",
      "Train accuracy:  0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/50, Loss: 0.6896:  50%|█████     | 2/4 [08:43<10:13, 306.91s/it]/var/folders/9v/r3fdnxqn6v740_k7q9q14pym0000gn/T/ipykernel_77584/4062299613.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:  tensor([[0.1694, 0.8306],\n",
      "        [0.1691, 0.8309],\n",
      "        [0.1675, 0.8325],\n",
      "        [0.1689, 0.8311],\n",
      "        [0.1645, 0.8355],\n",
      "        [0.1651, 0.8349],\n",
      "        [0.1659, 0.8341],\n",
      "        [0.1624, 0.8376]], grad_fn=<SoftmaxBackward0>)\n",
      "Index:  tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n",
      "Predicted label:  [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Ground truth:  tensor([0, 1, 1, 0, 0, 1, 0, 0])\n",
      "Train accuracy:  0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/50, Loss: 1.1903:  75%|███████▌  | 3/4 [17:21<06:43, 403.31s/it]/var/folders/9v/r3fdnxqn6v740_k7q9q14pym0000gn/T/ipykernel_77584/4062299613.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:  tensor([[0.8991, 0.1009],\n",
      "        [0.8984, 0.1016]], grad_fn=<SoftmaxBackward0>)\n",
      "Index:  tensor([[0],\n",
      "        [0]])\n",
      "Predicted label:  [[0]\n",
      " [0]]\n",
      "Ground truth:  tensor([1, 0])\n",
      "Train accuracy:  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/50, Loss: 1.2005: 100%|██████████| 4/4 [19:56<00:00, 299.03s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/var/folders/9v/r3fdnxqn6v740_k7q9q14pym0000gn/T/ipykernel_77584/4062299613.py:273: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n",
      "  0%|          | 0/4 [02:08<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  tensor(0)\n",
      "Ground truth:  tensor([1, 0, 0, 0, 1, 0, 1, 1])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 279\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGround truth: \u001b[39m\u001b[38;5;124m'\u001b[39m, ground_truth)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m#One image should match 1 label, but 1 label can match will multiple images (when single label classification)\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss_img(index,ground_truth) \n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Convert similarity scores to predicted labels\u001b[39;00m\n\u001b[1;32m    282\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # use CUDA device\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\") # use MacOS GPU device (e.g., for M2 chips)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # use CPU device\n",
    "print('Used device: ', device)\n",
    "\n",
    "#Load CLIP model - ViT B32\n",
    "model, preprocess = clip.load('ViT-B/16', device, jit=False)\n",
    "\n",
    "# Load the dataset\n",
    "class ImageTitleDataset(Dataset):\n",
    "    def __init__(self, list_video_path, list_labels, class_names, transform_image):\n",
    "        #to handle the parent class\n",
    "        super().__init__()\n",
    "        #Initalize image paths and corresponding texts\n",
    "        self.video_path = list_video_path\n",
    "        #Initialize labels (0 or 1)\n",
    "        self.labels = list_labels\n",
    "        #Initialize class names (no smoke or smoke)\n",
    "        self.class_names = class_names\n",
    "        #Transform to tensor\n",
    "        #self.transforms = ToTensor()\n",
    "        self.transform_image = transform_image\n",
    "\n",
    "    @staticmethod\n",
    "    #Function to create a square-shaped image from the video (similar to 1 long image)\n",
    "    #To do: what if the video has more frames than 36?\n",
    "    def preprocess_video_to_image_grid_version(video_path, num_rows=6, num_cols=6):\n",
    "        #Open the video file\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        #Create list for extracted frames\n",
    "        frames = []\n",
    "        #Handle if video can't be opened\n",
    "        if not video.isOpened():\n",
    "            print(\"Error: Could not open video file\")\n",
    "        else:\n",
    "            while True:\n",
    "                is_read, frame = video.read()\n",
    "                if not is_read:\n",
    "                    break\n",
    "                frames.append(frame)\n",
    "            video.release()\n",
    "        \n",
    "        if len(frames) != 36:\n",
    "            print(\"Num of frames are not 36\")\n",
    "            print(\"Num of frames for video on \", video_path, \"is \", len(frames))\n",
    "        \n",
    "        # Create  and store rows in the grids\n",
    "        rows_list = []\n",
    "        for i in range(num_rows):\n",
    "            #create rows from the frames using indexes -- for example, if i=0, then between the 0th and 6th frame\n",
    "            row = np.concatenate(frames[i * num_cols: (i + 1) * num_cols], axis=1)\n",
    "            rows_list.append(row)\n",
    "        \n",
    "        # Concatenate grid vertically to create a single square-shaped image from the smoke video\n",
    "        concatenated_frames = np.concatenate(rows_list, axis=0)\n",
    "        return concatenated_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #tranform videos into images and preprocess with clip's function\n",
    "        video_path = self.video_path[idx]\n",
    "        image = self.preprocess_video_to_image_grid_version(video_path)\n",
    "        image = Image.fromarray(image)\n",
    "        image = self.transform_image(image)\n",
    "        #image = preprocess(image)\n",
    "        #get the corresponding class names and tokenize\n",
    "        true_label = self.labels[idx]\n",
    "        label = self.class_names[true_label]\n",
    "        label = clip.tokenize(label, context_length=77, truncate=True)\n",
    "        return image, label, true_label\n",
    "    \n",
    "#Define training, validation and test data\n",
    "# Load the JSON metadata\n",
    "with open('data/datasets/experimental_ijmond_dataset.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('data/datasets/experimental_ijmond_dataset.json', 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "with open('data/datasets/experimental_ijmond_dataset.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "\n",
    "# Convert the datasets to a Pandas DataFrame\n",
    "train_data = pd.DataFrame(train_data)\n",
    "val_data = pd.DataFrame(val_data)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "\n",
    "\n",
    "# Prepare the list of video file paths and labels\n",
    "train_list_video_path = [os.path.join(\"data/ijmond_videos/\", f\"{fn}.mp4\") for fn in train_data['file_name']]\n",
    "train_list_labels = [int(label) for label in train_data['label']]\n",
    "val_list_video_path = [os.path.join(\"data/ijmond_videos/\", f\"{fn}.mp4\") for fn in val_data['file_name']]\n",
    "val_list_labels = [int(label) for label in val_data['label']]\n",
    "test_list_video_path = [os.path.join(\"data/ijmond_videos/\", f\"{fn}.mp4\") for fn in test_data['file_name']]\n",
    "test_list_labels = [int(label) for label in test_data['label']]\n",
    "\n",
    "#Define class names in a list - it needs prompt engineering\n",
    "#class_names = [\"a photo of a factory with no smoke\", \"a photo of a smoking factory\"] #1\n",
    "#class_names = [\"a series picture of a factory with a shut down chimney\", \"a series picture of a smoking factory chimney\"] #- 2\n",
    "#class_names = [\"a photo of factories with clear sky above chimney\", \"a photo of factories emiting smoke from chimney\"] #- 3\n",
    "#class_names = [\"a photo of a factory with no smoke\", \"a photo of a smoking factory\"] #- 4\n",
    "class_names = [\"a series picture of a factory with clear sky above chimney\", \"a series picture of a smoking factory\"] #- 5\n",
    "#class_names = [\"a series picture of a factory with no smoke\", \"a series picture of a smoking factory\"] #- 6\n",
    "#class_names = [\"a sequental photo of an industrial plant with clear sky above chimney, created from a video\", \"a sequental photo of an industrial plant emiting smoke from chimney, created from a video\"]# - 7\n",
    "#class_names = [\"a photo of a shut down chimney\", \"a photo of smoke chimney\"] #-8\n",
    "#class_names = [\"The industrial plant appears to be in a dormant state, with no smoke or emissions coming from its chimney. The air around the facility is clear and clean.\",\"The smokestack of the factory is emitting dark or gray smoke against the sky. The emissions may be a result of industrial activities within the facility.\"] #-9\n",
    "#class_names = [\"a photo of an industrial site with no visible signs of pollution\", \"a photo of a smokestack emitting smoke against the sky\"] #-10\n",
    "#class_names = ['no smoke', 'smoke']\n",
    "\n",
    "# Define input resolution\n",
    "input_resolution = (224, 224)\n",
    "\n",
    "# Define the transformation pipeline - from CLIP preprocessor without random crop augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(input_resolution, interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711])\n",
    "])\n",
    "\n",
    "# Create dataset and data loader for training, validation and testing\n",
    "train_dataset = ImageTitleDataset(train_list_video_path, train_list_labels, class_names, transform)\n",
    "val_dataset = ImageTitleDataset(val_list_video_path, val_list_labels, class_names, transform)\n",
    "test_dataset = ImageTitleDataset(test_list_video_path, test_list_labels, class_names, transform)\n",
    "\n",
    "print('Datasets created')\n",
    "\n",
    "#Create dataloader fot training and validation\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print('Dataloaders created')\n",
    "\n",
    "# Function to convert model's parameters to FP32 format\n",
    "#This is done so that our model loads in the provided memory\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "# Check if the device is set to CPU\n",
    "if device == \"cpu\":\n",
    "  model.float()\n",
    "\n",
    "# Prepare the optimizer - the lr, betas, eps and weight decay are from the CLIP paper\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "\n",
    "# Specify the loss functions - for images and for texts\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "# Model training\n",
    "num_epochs = 50\n",
    "print('starts training')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for batch in pbar:\n",
    "        # Extract images and texts from the batch\n",
    "        images, labels, true_label = batch \n",
    "\n",
    "        # Move images and texts to the specified device (CPU or GPU)\n",
    "        images= images.to(device)\n",
    "        texts = labels.to(device)\n",
    "        true_label = true_label.to(device)\n",
    "        text_inputs = clip.tokenize(class_names).to(device)\n",
    "\n",
    "        #Squeeze texts tensor to match the required size\n",
    "        texts = texts.squeeze(dim = 1)\n",
    "        text_inputs = text_inputs.squeeze(dim = 1)\n",
    "\n",
    "        # Forward pass - Run the model on the input data (images and texts)\n",
    "        logits_per_image, logits_per_text = model(images, text_inputs)\n",
    "\n",
    "        #Transform logits to float to match required dtype \n",
    "        logits_per_image = logits_per_image.float()\n",
    "        logits_per_text = logits_per_text.float()\n",
    "\n",
    "        #Ground truth\n",
    "        ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n",
    "\n",
    "        #Compute loss - contrastive loss to pull similar pairs closer together\n",
    "        #total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text.T,ground_truth))/2\n",
    "\n",
    "        #One image should match 1 label, but 1 label can match will multiple images (when single label classification)\n",
    "        total_loss = loss_img(logits_per_image, ground_truth) \n",
    "\n",
    "        # Get and convert similarity scores to predicted labels\n",
    "        similarity = logits_per_image.softmax(dim=-1)\n",
    "        value, index = similarity.topk(1)\n",
    "        print('Similarity: ', similarity)\n",
    "        print('Index: ',index)\n",
    "        \n",
    "        #Convert values to numpy\n",
    "        predicted_label = index.cpu().numpy()\n",
    "        ground_truth_label = ground_truth.cpu().numpy()\n",
    "        \n",
    "        print('Predicted label: ', predicted_label)\n",
    "        print('Ground truth: ', ground_truth)\n",
    "\n",
    "        train_accuracy = accuracy_score(ground_truth_label, predicted_label)\n",
    "        print('Train accuracy: ', train_accuracy)\n",
    "\n",
    "        # Zero out gradients for the optimizer (Adam) - to prevent adding gradients to previous ones\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            # Convert model's parameters to FP32 format, update, and convert back\n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "        # Update the progress bar with the current epoch and loss\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "      for batch in tqdm(val_dataloader, total=len(val_dataloader)):\n",
    "            # Extract images and texts from the batch\n",
    "            images, labels, true_label = batch \n",
    "            # Move images and texts to the specified device\n",
    "            images = images.to(device)\n",
    "            texts = labels.to(device)\n",
    "            true_label = true_label.to(device)\n",
    "            text_inputs = clip.tokenize(class_names).to(device)\n",
    "            texts = texts.squeeze(dim=1)\n",
    "            text_inputs.squeeze(dim=1)\n",
    "\n",
    "            # Forward pass\n",
    "            logits_per_image, logits_per_text = model(images, text_inputs)\n",
    "\n",
    "            #Transform logits to float to match required dtype \n",
    "            logits_per_image = logits_per_image.float()\n",
    "            logits_per_text = logits_per_text.float()\n",
    "\n",
    "            #logits_per_image, logits_per_text = model(images, text_inputs)\n",
    "            similarity = logits_per_image[0].softmax(dim=-1)\n",
    "\n",
    "            #values are the probabilities, indicies are the classes\n",
    "            value, index = similarity[0].topk(1)\n",
    "            ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n",
    "\n",
    "            print('Index: ', index)\n",
    "            print('Ground truth: ', ground_truth)\n",
    "\n",
    "            #One image should match 1 label, but 1 label can match will multiple images (when single label classification)\n",
    "            total_loss = loss_img(index,ground_truth) \n",
    "\n",
    "            # Convert similarity scores to predicted labels\n",
    "            predicted_label = index.cpu().numpy()\n",
    "            ground_truth_label = ground_truth.cpu().numpy()\n",
    "\n",
    "            # Append predicted labels and ground truth labels\n",
    "            all_preds.append(predicted_label)\n",
    "            all_labels.append(ground_truth_label)\n",
    "\n",
    "            # Append loss\n",
    "            val_losses.append(total_loss.item())\n",
    "\n",
    "            val_accuracy = accuracy_score(ground_truth_label, predicted_label)\n",
    "            print('Validation accuracy: ', val_accuracy)\n",
    "            \n",
    "            print(ground_truth_label, predicted_label)\n",
    "            val_precision = precision_score(ground_truth_label, predicted_label, average='binary')\n",
    "            print('Validation_precision: ', val_precision)\n",
    "  \n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Print or visualize the confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    print('Start testing')\n",
    "    def test_clip(dataset):\n",
    "        predicted_labels= []\n",
    "        ground_truths = []\n",
    "        # Loop over each image in dataloader\n",
    "        for rows in dataset:\n",
    "            \n",
    "            images, labels, true_label = rows\n",
    "\n",
    "            # Move images and texts to the specified device\n",
    "            images = images.unsqueeze(0).to(device)\n",
    "            texts = labels.to(device)\n",
    "            #true_label = true_label.to(device)\n",
    "            text_inputs = clip.tokenize(class_names).to(device)\n",
    "            texts = texts.squeeze(dim=1)\n",
    "            text_inputs.squeeze(dim=1)\n",
    "\n",
    "            # Calculate features\n",
    "            #with torch.no_grad():\n",
    "            #    image_features = model.encode_image(images)\n",
    "            #    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "            # Calculate similarity\n",
    "            #image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            #text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            #similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            #print(similarity)\n",
    "\n",
    "            #It's the same as the similarity:\n",
    "            logits_per_image, logits_per_text = model(images, text_inputs)\n",
    "            similarity = logits_per_image[0].softmax(dim=-1)\n",
    "\n",
    "            #values are the probabilities, indicies are the classes\n",
    "            value, index = similarity[0].topk(1)\n",
    "            ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n",
    "\n",
    "            # Convert similarity scores to predicted labels\n",
    "            predicted_label = index.cpu().numpy()\n",
    "            ground_truth_label = ground_truth.cpu().numpy()\n",
    "\n",
    "            predicted_labels.append(predicted_label)\n",
    "            ground_truths.append(ground_truth_label)\n",
    "    \n",
    "        # Compute accuracy\n",
    "        accuracy = accuracy_score(ground_truths, predicted_labels)\n",
    "\n",
    "        # Compute precision\n",
    "        precision = precision_score(ground_truths, predicted_labels, average='binary')\n",
    "\n",
    "        # Compute recall\n",
    "        recall = recall_score(ground_truths, predicted_labels, average='binary')\n",
    "\n",
    "        # Compute F1 score\n",
    "        f1 = f1_score(ground_truths, predicted_labels, average='binary')\n",
    "    \n",
    "        # Print or log the metrics\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Test Precision: {precision:.4f}\")\n",
    "        print(f\"Test Recall: {recall:.4f}\")\n",
    "        print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "test_clip(test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
