{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.mobilenet_v2(pretrained=True)\n",
    "#model\n",
    "#this model is more computationally heavy and less well-performing than the MobilenetV3 smalll model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for mobilenetV2 with LTSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class MobileNetV2_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dim=256, lstm_layers=1, bidirectional=False):\n",
    "        super(MobileNetV2_LSTM, self).__init__()\n",
    "        \n",
    "        #extract features from the mobilenet v2 model - output: 1280\n",
    "        self.feature_extractor = models.mobilenet_v2(pretrained=True, progress=True).features\n",
    "\n",
    "        #pooling - because we are using just the features (not containing the last pooling)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        #lstm layer\n",
    "        self.lstm = nn.LSTM(1280, hidden_dim, lstm_layers, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "        #linear layer to get prediction\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        \n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        c_out = self.feature_extractor(c_in)\n",
    "        print(\"c_out before pooling shape\", c_out.shape)\n",
    "        #shape: batch_size * timesteps, features, 1, 1\n",
    "        c_out = self.pool(c_out)\n",
    "        print(\"c_out after pooling shape\", c_out.shape)\n",
    "        #lstm layer needs a 3D tensor, with shape (batch, timesteps, feature)\n",
    "        c_out = c_out.view(batch_size, timesteps, -1)\n",
    "        print('c_out', c_out.shape)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(c_out)\n",
    "        \n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "#Example\n",
    "model = MobileNetV2_LSTM(num_classes=2)\n",
    "input_tensor = torch.randn(8, 16, 3, 224, 224)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)\n",
    "top1_prob, top1_catid = torch.topk(probabilities, 1)\n",
    "print(top1_prob, top1_catid)\n",
    "\n",
    "#it is not that computationally effective as it can crash the kernel when running on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNetV3 small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Dropout(p=0.2, inplace=True)\n",
       "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.mobilenet_v3_small(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2dNormActivation(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (2): Hardswish()\n",
       "  )\n",
       "  (1): InvertedResidual(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (scale_activation): Hardsigmoid()\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): InvertedResidual(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): InvertedResidual(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "        (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): InvertedResidual(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (2): SqueezeExcitation(\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (scale_activation): Hardsigmoid()\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (5): InvertedResidual(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (2): SqueezeExcitation(\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (scale_activation): Hardsigmoid()\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (6): InvertedResidual(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (2): SqueezeExcitation(\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (scale_activation): Hardsigmoid()\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (7): InvertedResidual(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (2): SqueezeExcitation(\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (scale_activation): Hardsigmoid()\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (8): InvertedResidual(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "        (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (2): SqueezeExcitation(\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (scale_activation): Hardsigmoid()\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (9): InvertedResidual(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "        (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (2): SqueezeExcitation(\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (scale_activation): Hardsigmoid()\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (10): InvertedResidual(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (2): SqueezeExcitation(\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (scale_activation): Hardsigmoid()\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (11): InvertedResidual(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (2): SqueezeExcitation(\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (scale_activation): Hardsigmoid()\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (12): Conv2dNormActivation(\n",
       "    (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (2): Hardswish()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examining just the features of the model\n",
    "model = models.mobilenet_v3_small(pretrained=True).features\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    #transforms.CenterCrop(224), #exlude center crop (the smoke can be anywhere)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for the MobilenetV3 small + LSTM/GRU model - trying out and understanding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class MobileNetV3Small_RNN(nn.Module):\n",
    "    def __init__(self, num_classes, rnn_type=\"LSTM\"):\n",
    "        super(MobileNetV3Small_RNN, self).__init__()\n",
    "        #load the model\n",
    "        self.mobilenet = models.mobilenet_v3_small(pretrained=True)\n",
    "\n",
    "        #extract features from final layer - pooling is exluded\n",
    "        self.feature_extractor = self.mobilenet.features\n",
    "\n",
    "        #Pooling\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        #get the number of features output by MobileNetV3 - for input for RNN\n",
    "        self.num_features = self.mobilenet.classifier[0].in_features\n",
    "\n",
    "        #room for rnn type choice: LSTM or GRU\n",
    "        if rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(self.num_features, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        elif rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(self.num_features, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "\n",
    "        #final classification layer - to get logits for the two classes\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "\n",
    "        #accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. \n",
    "        #reshape input for feature extraction - mobilenet can only take images (4 d)\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        \n",
    "        #extract features with mobilenet\n",
    "        features = self.feature_extractor(c_in)\n",
    "        \n",
    "        #pooling - using the same one as in the mobilenet architecture\n",
    "        #lstm layer needs a 3D tensor, with shape (batch, timesteps, feature)\n",
    "        features = self.pool(features).view(batch_size, timesteps, -1)\n",
    "\n",
    "        #get rnn output by passing the features to the selected rnn\n",
    "        rnn_out, _ = self.rnn(features)\n",
    "        \n",
    "        #batch, timesteps, output features\n",
    "        #only select the last of the timesteps as it holds the information of the whole video\n",
    "        last_output = rnn_out[:, -1, :]\n",
    "        logits = self.fc(last_output)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n",
      "tensor([[0.5143, 0.4857],\n",
      "        [0.4431, 0.5569],\n",
      "        [0.4896, 0.5104],\n",
      "        [0.4948, 0.5052],\n",
      "        [0.5098, 0.4902],\n",
      "        [0.4948, 0.5052],\n",
      "        [0.5107, 0.4893],\n",
      "        [0.5215, 0.4785]], grad_fn=<SoftmaxBackward0>)\n",
      "Class:  tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]]) : tensor([[0.5143],\n",
      "        [0.5569],\n",
      "        [0.5104],\n",
      "        [0.5052],\n",
      "        [0.5098],\n",
      "        [0.5052],\n",
      "        [0.5107],\n",
      "        [0.5215]], grad_fn=<TopkBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9v/r3fdnxqn6v740_k7q9q14pym0000gn/T/ipykernel_29050/699404125.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilities = torch.nn.functional.softmax(predictions)\n"
     ]
    }
   ],
   "source": [
    "#Experiment: rnn_type can be LSTM and GRU\n",
    "model = MobileNetV3Small_RNN(num_classes=2, rnn_type=\"GRU\")\n",
    "# Modelling the merged dataset: batchsize of 8, 36 frames per video, rgb, 224x224 frames (needs preprocessing)\n",
    "video_tensor = torch.randn(8, 36, 3, 224, 224)  \n",
    "predictions = model(video_tensor)\n",
    "\n",
    "print(predictions.size())\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(predictions)\n",
    "print(probabilities)\n",
    "top1_prob, top1_catid = torch.topk(probabilities, 1)\n",
    "print(\"Class: \", top1_catid, \":\", top1_prob)\n",
    "\n",
    "#LSTM runs for 7.3s\n",
    "#GRU runs for 8.4 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining EfficientNetB0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9v/r3fdnxqn6v740_k7q9q14pym0000gn/T/ipykernel_29050/667827384.py:58: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilities = torch.nn.functional.softmax(predictions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4682, 0.5318],\n",
      "        [0.4987, 0.5013],\n",
      "        [0.4693, 0.5307],\n",
      "        [0.4363, 0.5637],\n",
      "        [0.5007, 0.4993],\n",
      "        [0.5080, 0.4920],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.4742, 0.5258]], grad_fn=<SoftmaxBackward0>)\n",
      "Class:  tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1]]) : tensor([[0.5318],\n",
      "        [0.5013],\n",
      "        [0.5307],\n",
      "        [0.5637],\n",
      "        [0.5007],\n",
      "        [0.5080],\n",
      "        [0.5153],\n",
      "        [0.5258]], grad_fn=<TopkBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class EfficientNetB0_RNN(nn.Module):\n",
    "    def __init__(self, num_classes, rnn_type=\"LSTM\"):\n",
    "        super(EfficientNetB0_RNN, self).__init__()\n",
    "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "        #feature extraction without last pooling layer\n",
    "        self.feature_extractor = self.efficientnet.features\n",
    "\n",
    "        #get number of features output by model\n",
    "        self.num_features = self.efficientnet.classifier[1].in_features\n",
    "\n",
    "        #define the rnn layer type\n",
    "        if rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(self.num_features, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        elif rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(self.num_features, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "\n",
    "        #classification layer to get logits\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "\n",
    "        #reshape to fit the cnn input requirements\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        \n",
    "        #extract features\n",
    "        features = self.feature_extractor(c_in)\n",
    "        \n",
    "        #pooling and reshaping to get (batch, timesteps, features) for rnn\n",
    "        features = nn.functional.adaptive_avg_pool2d(features, (1, 1)).reshape(batch_size, timesteps, -1)\n",
    "\n",
    "        #rnn layer\n",
    "        rnn_out, _ = self.rnn(features)\n",
    "\n",
    "        #only get the last element of timesteps\n",
    "        last_output = rnn_out[:, -1, :]\n",
    "\n",
    "        #pass thorugh fully connected layer to get logits\n",
    "        logits = self.fc(last_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "#try model with random example \n",
    "model = EfficientNetB0_RNN(num_classes=2, rnn_type=\"GRU\")\n",
    "video_tensor = torch.randn(8, 16, 3, 224, 224)\n",
    "predictions = model(video_tensor)\n",
    "\n",
    "#print out size - should be batch,classes\n",
    "print(predictions.size())\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(predictions)\n",
    "print(probabilities)\n",
    "top1_prob, top1_catid = torch.topk(probabilities, 1)\n",
    "print(\"Class: \", top1_catid, \":\", top1_prob)\n",
    "\n",
    "#for LSTM: 26s\n",
    "#for GRU: 46.4s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: MobilenetV3 small is significantly more efficient, the LSTM layer seems more efficient, too. I will experiment with MobileNetV3 small with LSTM or GRU layers to find the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating code for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Define device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # use CUDA device\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\") # use MacOS GPU device (e.g., for M2 chips)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # use CPU device\n",
    "print('Used device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV3Small_RNN(num_classes=2, rnn_type=\"LSTM\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "class ImageTitleDataset(Dataset):\n",
    "    def __init__(self, list_video_path, list_labels, transform_image):\n",
    "        #to handle the parent class\n",
    "        super().__init__()\n",
    "        #Initalize image paths and corresponding texts\n",
    "        self.video_path = list_video_path\n",
    "        #Initialize labels (0 or 1)\n",
    "        self.labels = list_labels\n",
    "        #Transform images based on defined transformation\n",
    "        self.transform_image = transform_image\n",
    "\n",
    "    @staticmethod\n",
    "    #Function to create a square-shaped image from the video (similar to 1 long image)\n",
    "\n",
    "    def preprocess_video_to_a_set_of_images(video_path):\n",
    "        #Open the video file\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        #Create list for extracted frames\n",
    "        frames = []\n",
    "        #Handle if video can't be opened\n",
    "        if not video.isOpened():\n",
    "            print(\"Error: Could not open video file\")\n",
    "        else:\n",
    "            while True:\n",
    "                is_read, frame = video.read()\n",
    "                if not is_read:\n",
    "                    break\n",
    "                frames.append(frame)\n",
    "            video.release()\n",
    "        \n",
    "        if len(frames) != 36:\n",
    "            print(\"Num of frames are not 36\")\n",
    "            print(\"Num of frames for video on \", video_path, \"is \", len(frames))\n",
    "        \n",
    "        return frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #tranform videos into images and preprocess with defined transform function\n",
    "        video_path = self.video_path[idx]\n",
    "        frames = self.preprocess_video_to_a_set_of_images(video_path)\n",
    "        frames = [self.transform_image(Image.fromarray(frame)) for frame in frames]\n",
    "        frames = torch.stack(frames)\n",
    "\n",
    "        #get the corresponding class names\n",
    "        label = self.labels[idx]\n",
    "        return frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training data\n",
    "# Load the JSON metadata\n",
    "with open('data/datasets/experimental_ijmond_dataset.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "train_data = pd.DataFrame(train_data)\n",
    "# Prepare the list of video file paths and labels\n",
    "list_video_path = [os.path.join(\"data/ijmond_videos/\", f\"{fn}.mp4\") for fn in train_data['file_name']]\n",
    "#list_labels = dataset['label'].tolist()\n",
    "list_labels = [int(label) for label in train_data['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input resolution\n",
    "input_resolution = (256, 256)\n",
    "\n",
    "# Define the transformation pipeline - from CLIP preprocessor without random crop augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(input_resolution, interpolation=Image.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.RandomPerspective(distortion_scale=0.3, p=0.3),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and data loader for training\n",
    "train_dataset = ImageTitleDataset(list_video_path, list_labels, transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and data loader for val\n",
    "val_dataset = ImageTitleDataset(list_video_path, list_labels, transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and data loader for test\n",
    "test_dataset = ImageTitleDataset(list_video_path, list_labels, transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Images shape: torch.Size([4, 36, 3, 256, 256])\n",
      "Labels: tensor([1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "#inspect the first batch in the DataLoader\n",
    "def inspect_dataloader(dataloader):\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        print(f\"Batch {batch_idx + 1}\")\n",
    "        print(f\"Images shape: {images.shape}\")\n",
    "        # Should be (4, 36, 3, 224, 224) - batch_size, frames, rgb, width, height\n",
    "        print(f\"Labels: {labels}\")\n",
    "        break\n",
    "\n",
    "inspect_dataloader(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 36, 3, 256, 256])\n",
      "tensor([1, 0, 0, 1])\n",
      "Train accuracy:  0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Loss: 0.6662:  14%|        | 1/7 [00:15<01:35, 15.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 36, 3, 256, 256])\n",
      "tensor([0, 0, 0, 0])\n",
      "Train accuracy:  0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Loss: 0.8104:  29%|       | 2/7 [00:29<01:12, 14.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 36, 3, 256, 256])\n",
      "tensor([0, 0, 0, 0])\n",
      "Train accuracy:  0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Loss: 0.8104:  29%|       | 2/7 [00:43<01:49, 21.87s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain accuracy: \u001b[39m\u001b[38;5;124m'\u001b[39m, train_accuracy)\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "best_te_loss = 1e5\n",
    "best_ep = -1\n",
    "early_stopping_counter = 0\n",
    "early_stopping_patience = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"running epoch {epoch}, best test loss {best_te_loss} after epoch {best_ep}\")\n",
    "    step = 0\n",
    "    tr_loss = 0\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for batch in pbar:\n",
    "        step += 1\n",
    "\n",
    "        # Extract images and labels from the batch\n",
    "        images, labels = batch \n",
    "        print(images.shape)\n",
    "\n",
    "        # Move images and texts to the specified device (CPU or GPU)\n",
    "        images= images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        predictions = model(images)\n",
    "        batch_loss = loss(predictions, labels)\n",
    "        tr_loss += batch_loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        probabilities = torch.argmax(predictions, dim=1)\n",
    "        print(probabilities)\n",
    "        train_accuracy = accuracy_score(labels.cpu().numpy(), probabilities.cpu().numpy())\n",
    "        print('Train accuracy: ', train_accuracy)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {batch_loss.item():.4f}, Current Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    tr_loss /= step\n",
    "\n",
    "    print('Validation loop starts')\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    te_loss = 0\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        vbar = tqdm(val_dataloader, total=len(val_dataloader))\n",
    "        i = 0\n",
    "        for batch in vbar:\n",
    "            step += 1\n",
    "            images, labels = batch\n",
    "            images= images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(images)\n",
    "            val_loss = loss(predictions, labels)\n",
    "            te_loss += val_loss.item()\n",
    "\n",
    "            pred_labels = predictions.argmax(dim=1)\n",
    "\n",
    "            # Append predicted labels and ground truth labels\n",
    "            all_preds.extend(pred_labels.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Append loss\n",
    "            val_losses.append(val_loss.item())\n",
    "        \n",
    "            # Update the progress bar with the current epoch and loss\n",
    "            vbar.set_description(f\"Validation: {i}/{len(val_dataloader)}, Validation loss: {val_loss.item():.4f}\")\n",
    "            i+=1\n",
    "\n",
    "        te_loss /= step\n",
    "    \n",
    "     # Convert lists of arrays to numpy arrays\n",
    "    all_labels_array = np.concatenate(all_labels)\n",
    "    all_preds_array = np.concatenate(all_preds)\n",
    "\n",
    "    # Convert to 1D arrays\n",
    "    all_labels_flat = all_labels_array.flatten()\n",
    "    all_preds_flat = all_preds_array.flatten()\n",
    "\n",
    "    # Ensure they are integers\n",
    "    all_labels_int = all_labels_flat.astype(int)\n",
    "    all_preds_int = all_preds_flat.astype(int)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels_int, all_preds_int)\n",
    "\n",
    "    # Print or visualize the confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    #get evaluation metrics:\n",
    "\n",
    "    precision = precision_score(all_labels_int, all_preds_int, average='binary')\n",
    "    recall = recall_score(all_labels_int, all_preds_int, average='binary')\n",
    "    f_score= f1_score(all_labels_int, all_preds_int, average='binary')\n",
    "    acc = accuracy_score(all_labels_int, all_preds_int)\n",
    "\n",
    "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "    print(f\"Validation Precision: {precision:.4f}\")\n",
    "    print(f\"Validation Recall: {recall:.4f}\")\n",
    "    print(f\"Validation F1 Score: {f_score:.4f}\")\n",
    "\n",
    "    if te_loss < best_te_loss:\n",
    "        best_te_loss = te_loss\n",
    "        best_ep = epoch\n",
    "        torch.save(model.state_dict(), \"../light_cnn_best_model.pt\")\n",
    "        early_stopping_counter = 0 \n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    print(f\"epoch {epoch}, tr_loss {tr_loss}, te_loss {te_loss}\")\n",
    "\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        print(f\"Early stopping after {epoch + 1} epochs.\")\n",
    "        break\n",
    "    \n",
    "print(f\"best epoch {best_ep+1}, best te_loss {best_te_loss}\")\n",
    "torch.save(model.state_dict(), \"../light_cnn_last_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:08<?, ?it/s]\n",
      " 43%|     | 3/7 [00:12<00:17,  4.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m images\u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     15\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m loss(predictions, labels)\n\u001b[1;32m     17\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m, in \u001b[0;36mMobileNetV3Small_RNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m c_in \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m timesteps, C, H, W)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#extract features with mobilenet\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(c_in)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#pooling - using the same one as in the mobilenet architecture\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#lstm layer needs a 3D tensor, with shape (batch, timesteps, feature)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(features)\u001b[38;5;241m.\u001b[39mview(batch_size, timesteps, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/mobilenetv3.py:111\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 111\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[1;32m    113\u001b[0m         result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"start testing\")\n",
    "model.eval()\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    tbar = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    i = 0\n",
    "    for batch in tbar:\n",
    "        images, labels = batch\n",
    "        images= images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        predictions = model(images)\n",
    "        test_loss = loss(predictions, labels)\n",
    "\n",
    "        pred_labels = predictions.argmax(dim=1)\n",
    "\n",
    "        # Append predicted labels and ground truth labels\n",
    "        test_preds.extend(pred_labels.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Append loss\n",
    "        test_losses.append(test_loss.item())\n",
    "    \n",
    "    # Update the progress bar with the current epoch and loss\n",
    "    tbar.set_description(f\"Testing: {i}/{len(test_dataloader)}, Test loss: {test_loss.item():.4f}\")\n",
    "    i+=1\n",
    "\n",
    "# Convert lists of arrays to numpy arrays\n",
    "all_labels_array = np.concatenate(test_labels)\n",
    "all_preds_array = np.concatenate(test_preds)\n",
    "\n",
    "# Convert to 1D arrays\n",
    "all_labels_flat = all_labels_array.flatten()\n",
    "all_preds_flat = all_preds_array.flatten()\n",
    "\n",
    "# Ensure they are integers\n",
    "all_labels_int = all_labels_flat.astype(int)\n",
    "all_preds_int = all_preds_flat.astype(int)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels_int, all_preds_int)\n",
    "\n",
    "# Print or visualize the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "#get evaluation metrics:\n",
    "\n",
    "precision = precision_score(all_labels_int, all_preds_int, average='binary')\n",
    "recall = recall_score(all_labels_int, all_preds_int, average='binary')\n",
    "f_score= f1_score(all_labels_int, all_preds_int, average='binary')\n",
    "acc = accuracy_score(all_labels_int, all_preds_int)\n",
    "\n",
    "# Print or log the metrics\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f_score:.4f}\")\n",
    "\n",
    "print(\"CLIP model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "\n",
    "# Classification report\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(all_labels_int, all_preds_int , target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used device:  cpu\n",
      "Datasets created\n",
      "Dataloaders created\n",
      "running epoch 0, best test loss 100000.0 after epoch -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 36, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Loss: 0.7322, Current Learning rate: 0.1:  25%|       | 1/4 [00:32<01:36, 32.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 36, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Loss: 3.2667, Current Learning rate: 0.1:  50%|     | 2/4 [01:01<01:01, 30.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 36, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Loss: 7.1069, Current Learning rate: 0.1:  75%|  | 3/4 [01:29<00:29, 29.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 36, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Loss: 2.6335, Current Learning rate: 0.1: 100%|| 4/4 [01:35<00:00, 23.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.4230769230769231\n",
      "Validation loop starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 0/1, Validation loss: 6.3810: 100%|| 1/1 [00:22<00:00, 22.09s/it]\n",
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.5\n",
      "Confusion Matrix:\n",
      "[[13  0]\n",
      " [13  0]]\n",
      "Validation Accuracy: 0.5000\n",
      "Validation Precision: 0.0000\n",
      "Validation Recall: 0.0000\n",
      "Validation F1 Score: 0.0000\n",
      "epoch 0, tr_loss 3.4348270893096924, te_loss 6.380963325500488\n",
      "best epoch 1, best te_loss 6.380963325500488\n",
      "start testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 0/1, Test loss: 6.3810: 100%|| 1/1 [00:21<00:00, 21.68s/it]\n",
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2024-05-24 00:11:13.731959\n",
      "Ending time:  2024-05-24 00:11:35.415512\n",
      "Overall time:  0:00:21.683553\n",
      "Confusion Matrix:\n",
      "[[13  0]\n",
      " [13  0]]\n",
      "Test Accuracy: 0.5000\n",
      "Test Precision: 0.0000\n",
      "Test Recall: 0.0000\n",
      "Test F1 Score: 0.0000\n",
      "CLIP model parameters: 3,397,386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.50      1.00      0.67        13\n",
      "     class 1       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.50        26\n",
      "   macro avg       0.25      0.50      0.33        26\n",
      "weighted avg       0.25      0.50      0.33        26\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import packages\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Define device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # use CUDA device\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\") # use MacOS GPU device (e.g., for M2 chips)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # use CPU device\n",
    "print('Used device: ', device)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class MobileNetV3Small_RNN(nn.Module):\n",
    "    def __init__(self, num_classes, rnn_type=\"LSTM\"):\n",
    "        super(MobileNetV3Small_RNN, self).__init__()\n",
    "        #load the model\n",
    "        self.mobilenet = models.mobilenet_v3_small(pretrained=True)\n",
    "\n",
    "        #extract features from final layer - pooling is exluded\n",
    "        self.feature_extractor = self.mobilenet.features\n",
    "\n",
    "        #Pooling\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        #get the number of features output by MobileNetV3 - for input for RNN\n",
    "        self.num_features = self.mobilenet.classifier[0].in_features\n",
    "\n",
    "        #room for rnn type choice: LSTM or GRU\n",
    "        if rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(self.num_features, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        elif rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(self.num_features, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "\n",
    "        #final classification layer - to get logits for the two classes\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "\n",
    "        #accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. \n",
    "        #reshape input for feature extraction - mobilenet can only take images (4 d)\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        \n",
    "        #extract features with mobilenet\n",
    "        features = self.feature_extractor(c_in)\n",
    "        \n",
    "        #pooling - using the same one as in the mobilenet architecture\n",
    "        #lstm layer needs a 3D tensor, with shape (batch, timesteps, feature)\n",
    "        features = self.pool(features).view(batch_size, timesteps, -1)\n",
    "\n",
    "        #get rnn output by passing the features to the selected rnn\n",
    "        rnn_out, _ = self.rnn(features)\n",
    "        \n",
    "        #batch, timesteps, output features\n",
    "        #only select the last of the timesteps as it holds the information of the whole video\n",
    "        last_output = rnn_out[:, -1, :]\n",
    "        logits = self.fc(last_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "model = MobileNetV3Small_RNN(num_classes=2, rnn_type=\"LSTM\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the dataset\n",
    "class ImageTitleDataset(Dataset):\n",
    "    def __init__(self, list_video_path, list_labels, transform_image):\n",
    "        #to handle the parent class\n",
    "        super().__init__()\n",
    "        #Initalize image paths and corresponding texts\n",
    "        self.video_path = list_video_path\n",
    "        #Initialize labels (0 or 1)\n",
    "        self.labels = list_labels\n",
    "        #Transform images based on defined transformation\n",
    "        self.transform_image = transform_image\n",
    "\n",
    "    @staticmethod\n",
    "    #Function to create a square-shaped image from the video (similar to 1 long image)\n",
    "\n",
    "    def preprocess_video_to_a_set_of_images(video_path):\n",
    "        #Open the video file\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        #Create list for extracted frames\n",
    "        frames = []\n",
    "        #Handle if video can't be opened\n",
    "        if not video.isOpened():\n",
    "            print(\"Error: Could not open video file\")\n",
    "        else:\n",
    "            while True:\n",
    "                is_read, frame = video.read()\n",
    "                if not is_read:\n",
    "                    break\n",
    "                frames.append(frame)\n",
    "            video.release()\n",
    "        \n",
    "        if len(frames) != 36:\n",
    "            print(\"Num of frames are not 36\")\n",
    "            print(\"Num of frames for video on \", video_path, \"is \", len(frames))\n",
    "        \n",
    "        return frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #tranform videos into images and preprocess with defined transform function\n",
    "        video_path = self.video_path[idx]\n",
    "        frames = self.preprocess_video_to_a_set_of_images(video_path)\n",
    "        frames = [self.transform_image(Image.fromarray(frame)) for frame in frames]\n",
    "        frames = torch.stack(frames)\n",
    "\n",
    "        #get the corresponding class names\n",
    "        label = self.labels[idx]\n",
    "        return frames, label\n",
    "    \n",
    "\n",
    "#Define training, validation and test data\n",
    "# Load the JSON metadata\n",
    "with open('data/datasets/experimental_ijmond_dataset.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('data/datasets/experimental_ijmond_dataset.json', 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "with open('data/datasets/experimental_ijmond_dataset.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Convert the datasets to a Pandas DataFrame\n",
    "train_data = pd.DataFrame(train_data)\n",
    "val_data = pd.DataFrame(val_data)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "\n",
    "# Prepare the list of video file paths and labels\n",
    "train_list_video_path = [os.path.join(\"data/ijmond_videos/\", f\"{fn}.mp4\") for fn in train_data['file_name']]\n",
    "train_list_labels = [int(label) for label in train_data['label']]\n",
    "val_list_video_path = [os.path.join(\"data/ijmond_videos/\", f\"{fn}.mp4\") for fn in val_data['file_name']]\n",
    "val_list_labels = [int(label) for label in val_data['label']]\n",
    "test_list_video_path = [os.path.join(\"data/ijmond_videos/\", f\"{fn}.mp4\") for fn in test_data['file_name']]\n",
    "test_list_labels = [int(label) for label in test_data['label']]\n",
    "\n",
    "# Define input resolution\n",
    "input_resolution = (256, 256)\n",
    "\n",
    "# Define the transformation pipeline - from CLIP preprocessor without random crop augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(input_resolution, interpolation=Image.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.RandomPerspective(distortion_scale=0.3, p=0.3),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(input_resolution, interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset and data loader for training, validation and testing\n",
    "train_dataset = ImageTitleDataset(train_list_video_path, train_list_labels, train_transform)\n",
    "val_dataset = ImageTitleDataset(val_list_video_path, val_list_labels, val_test_transform)\n",
    "test_dataset = ImageTitleDataset(test_list_video_path, test_list_labels, val_test_transform)\n",
    "\n",
    "print('Datasets created')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print('Dataloaders created')\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1,betas=(0.9,0.98),eps=1e-6,weight_decay=1e-6)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader)*num_epochs)\n",
    "\n",
    "num_epochs = 1\n",
    "best_te_loss = 1e5\n",
    "best_ep = -1\n",
    "early_stopping_counter = 0\n",
    "early_stopping_patience = 4\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"running epoch {epoch}, best test loss {best_te_loss} after epoch {best_ep}\")\n",
    "    step = 0\n",
    "    tr_loss = 0\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    epoch_train_correct = 0\n",
    "    epoch_train_total = 0\n",
    "    for batch in pbar:\n",
    "        step += 1\n",
    "\n",
    "        # Extract images and labels from the batch\n",
    "        images, labels = batch \n",
    "        print(images.shape)\n",
    "\n",
    "        # Move images and texts to the specified device (CPU or GPU)\n",
    "        images= images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        predictions = model(images)\n",
    "        batch_loss = loss(predictions, labels)\n",
    "        tr_loss += batch_loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        probabilities = torch.argmax(predictions, dim=1)\n",
    "\n",
    "        correct = (probabilities == labels).sum().item()\n",
    "        total = labels.size(0)\n",
    "        epoch_train_correct += correct\n",
    "        epoch_train_total += total\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {batch_loss.item():.4f}, Current Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    tr_loss /= step\n",
    "    train_accuracy = epoch_train_correct / epoch_train_total\n",
    "    print('Train accuracy: ', train_accuracy)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_losses.append(tr_loss)\n",
    "\n",
    "    print('Validation loop starts')\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    te_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    epoch_val_correct = 0\n",
    "    epoch_val_total = 0\n",
    "    with torch.no_grad():\n",
    "        vbar = tqdm(val_dataloader, total=len(val_dataloader))\n",
    "        i = 0\n",
    "        for batch in vbar:\n",
    "            step += 1\n",
    "            images, labels = batch\n",
    "            images= images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(images)\n",
    "            val_loss = loss(predictions, labels)\n",
    "            te_loss += val_loss.item()\n",
    "\n",
    "            pred_labels = predictions.argmax(dim=1)\n",
    "\n",
    "            correct = (pred_labels == labels).sum().item()\n",
    "            total = labels.size(0)\n",
    "            epoch_val_correct += correct\n",
    "            epoch_val_total += total\n",
    "\n",
    "            # Append predicted labels and ground truth labels\n",
    "            all_preds.extend(pred_labels.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Append loss\n",
    "            val_losses.append(val_loss.item())\n",
    "        \n",
    "            # Update the progress bar with the current epoch and loss\n",
    "            vbar.set_description(f\"Validation: {i}/{len(val_dataloader)}, Validation loss: {val_loss.item():.4f}\")\n",
    "            i+=1\n",
    "\n",
    "        te_loss /= step\n",
    "        val_accuracy = epoch_val_correct / epoch_val_total\n",
    "        print(\"Validation accuracy: \", val_accuracy)\n",
    "        val_losses.append(te_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        train_losses.append(te_loss)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Print or visualize the confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    #get evaluation metrics:\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f_score= f1_score(all_labels, all_preds, average='binary')\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "    print(f\"Validation Precision: {precision:.4f}\")\n",
    "    print(f\"Validation Recall: {recall:.4f}\")\n",
    "    print(f\"Validation F1 Score: {f_score:.4f}\")\n",
    "\n",
    "    if te_loss < best_te_loss:\n",
    "        best_te_loss = te_loss\n",
    "        best_ep = epoch\n",
    "        torch.save(model.state_dict(), \"../light_cnn_best_model.pt\")\n",
    "        early_stopping_counter = 0 \n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    print(f\"epoch {epoch}, tr_loss {tr_loss}, te_loss {te_loss}\")\n",
    "\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        print(f\"Early stopping after {epoch + 1} epochs.\")\n",
    "        break\n",
    "    \n",
    "print(f\"best epoch {best_ep+1}, best te_loss {best_te_loss}\")\n",
    "torch.save(model.state_dict(), \"../light_cnn_last_model.pt\")\n",
    "\n",
    "print(\"start testing\")\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    start_time = datetime.now()\n",
    "    tbar = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    i = 0\n",
    "    for batch in tbar:\n",
    "        images, labels = batch\n",
    "        images= images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        predictions = model(images)\n",
    "        test_loss = loss(predictions, labels)\n",
    "\n",
    "        pred_labels = predictions.argmax(dim=1)\n",
    "\n",
    "        # Append predicted labels and ground truth labels\n",
    "        test_preds.extend(pred_labels.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "        # Update the progress bar with the current epoch and loss\n",
    "        tbar.set_description(f\"Testing: {i}/{len(test_dataloader)}, Test loss: {test_loss.item():.4f}\")\n",
    "        i+=1\n",
    "    end_time = datetime.now()\n",
    "    print('Start time: ', start_time)\n",
    "    print('Ending time: ', end_time)\n",
    "    print('Overall time: ', end_time-start_time)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Print or visualize the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "#get evaluation metrics:\n",
    "\n",
    "precision = precision_score(all_labels, all_preds, average='binary')\n",
    "recall = recall_score(all_labels, all_preds, average='binary')\n",
    "f_score= f1_score(all_labels, all_preds, average='binary')\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# Print or log the metrics\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f_score:.4f}\")\n",
    "\n",
    "print(\"CLIP model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "\n",
    "# Classification report\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(all_labels, all_preds , target_names=target_names))\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Loss')\n",
    "plt.plot(val_accuracies, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created\n",
      "Dataloaders created\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Adam.__init__() got an unexpected keyword argument 'decay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 213\u001b[0m\n\u001b[1;32m    211\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m#optimizer = torch.optim.Adam(model.parameters(), lr=5e-4,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m    214\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader)*num_epochs)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Adam.__init__() got an unexpected keyword argument 'decay'"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torchvision.io import read_video\n",
    "\n",
    "\n",
    "# Define device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # use CUDA device\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\") # use MacOS GPU device (e.g., for M2 chips)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # use CPU device\n",
    "print('Used device: ', device)\n",
    "\n",
    "class MobileNetV3Small_RNN(nn.Module):\n",
    "    def __init__(self, num_classes, rnn_type=\"LSTM\"):\n",
    "        super(MobileNetV3Small_RNN, self).__init__()\n",
    "        #load the model\n",
    "        self.mobilenet = models.mobilenet_v3_small(pretrained=True)\n",
    "\n",
    "        #freeze the mobilenet parameters (not training these for efficiency)\n",
    "        for param in self.mobilenet.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        #extract features from final layer - pooling is exluded\n",
    "        self.feature_extractor = self.mobilenet.features\n",
    "\n",
    "        #Pooling\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        #get the number of features output by MobileNetV3 - for input for RNN\n",
    "        self.num_features = self.mobilenet.classifier[0].in_features\n",
    "\n",
    "        #room for rnn type choice: LSTM or GRU\n",
    "        if rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(self.num_features, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        elif rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(self.num_features, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "\n",
    "        #final classification layer - to get logits for the two classes\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "\n",
    "        #accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. \n",
    "        #reshape input for feature extraction - mobilenet can only take images (4 d)\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        \n",
    "        #extract features with mobilenet\n",
    "        features = self.feature_extractor(c_in)\n",
    "        \n",
    "        #pooling - using the same one as in the mobilenet architecture\n",
    "        #lstm layer needs a 3D tensor, with shape (batch, timesteps, feature)\n",
    "        features = self.pool(features).view(batch_size, timesteps, -1)\n",
    "\n",
    "        #get rnn output by passing the features to the selected rnn\n",
    "        rnn_out, _ = self.rnn(features)\n",
    "        \n",
    "        #batch, timesteps, output features\n",
    "        #only select the last of the timesteps as it holds the information of the whole video\n",
    "        last_output = rnn_out[:, -1, :]\n",
    "        logits = self.fc(last_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "model = MobileNetV3Small_RNN(num_classes=2, rnn_type=\"LSTM\")\n",
    "model = model.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = DataParallel(model)\n",
    "\n",
    "# Load the dataset\n",
    "class ImageTitleDataset(Dataset):\n",
    "    def __init__(self, list_video_path, list_labels, transform_image):\n",
    "        #to handle the parent class\n",
    "        super().__init__()\n",
    "        #Initalize image paths and corresponding texts\n",
    "        self.video_path = list_video_path\n",
    "        #Initialize labels (0 or 1)\n",
    "        self.labels = list_labels\n",
    "        #Transform images based on defined transformation\n",
    "        self.transform_image = transform_image\n",
    "\n",
    "    @staticmethod\n",
    "    #Function to create a square-shaped image from the video (similar to 1 long image)\n",
    "\n",
    "    def preprocess_video_to_a_set_of_images(video_path):\n",
    "        #Open the video file\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        #Create list for extracted frames\n",
    "        frames = []\n",
    "        #Handle if video can't be opened\n",
    "        if not video.isOpened():\n",
    "            print(\"Error: Could not open video file\")\n",
    "        else:\n",
    "            while True:\n",
    "                is_read, frame = video.read()\n",
    "                if not is_read:\n",
    "                    break\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame_rgb)\n",
    "            video.release()\n",
    "        \n",
    "        if len(frames) != 36:\n",
    "            print(\"Num of frames are not 36\")\n",
    "            print(\"Num of frames for video on \", video_path, \"is \", len(frames))\n",
    "        \n",
    "        return frames\n",
    "\n",
    "    def preprocess_videos(self, video_path):\n",
    "        # Use torchvision's read_video function\n",
    "        frames, _, _ = read_video(video_path)\n",
    "        return frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #tranform videos into images and preprocess with defined transform function\n",
    "        video_path = self.video_path[idx]\n",
    "        frames = self.preprocess_videos(video_path)\n",
    "        frames = [self.transform_image(Image.fromarray(frame)) for frame in frames]\n",
    "        frames = torch.stack(frames)\n",
    "\n",
    "        #get the corresponding class names\n",
    "        label = self.labels[idx]\n",
    "        return frames, label\n",
    "    \n",
    "\n",
    "#Define training, validation and test data\n",
    "# Load the JSON metadata\n",
    "with open('data/split/metadata_train_split_by_date.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('data/split/metadata_validation_split_by_date.json', 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "with open('data/split/metadata_test_split_by_date.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Convert the datasets to a Pandas DataFrame\n",
    "train_data = pd.DataFrame(train_data)\n",
    "val_data = pd.DataFrame(val_data)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "\n",
    "# Prepare the list of video file paths and labels\n",
    "train_list_video_path = [os.path.join(\"/../projects/0/prjs0930/data/merged_videos/\", f\"{fn}.mp4\") for fn in train_data['file_name']]\n",
    "train_list_labels = [int(label) for label in train_data['label']]\n",
    "val_list_video_path = [os.path.join(\"/../projects/0/prjs0930/data/merged_videos/\", f\"{fn}.mp4\") for fn in val_data['file_name']]\n",
    "val_list_labels = [int(label) for label in val_data['label']]\n",
    "test_list_video_path = [os.path.join(\"/../projects/0/prjs0930/data/merged_videos/\", f\"{fn}.mp4\") for fn in test_data['file_name']]\n",
    "test_list_labels = [int(label) for label in test_data['label']]\n",
    "\n",
    "# Define input resolution\n",
    "input_resolution = (256, 256)\n",
    "\n",
    "# Define the transformation pipeline - from CLIP preprocessor without random crop augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(input_resolution, interpolation=Image.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.RandomPerspective(distortion_scale=0.3, p=0.3),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(input_resolution, interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset and data loader for training, validation and testing\n",
    "train_dataset = ImageTitleDataset(train_list_video_path, train_list_labels, train_transform)\n",
    "val_dataset = ImageTitleDataset(val_list_video_path, val_list_labels, val_test_transform)\n",
    "test_dataset = ImageTitleDataset(test_list_video_path, test_list_labels, val_test_transform)\n",
    "\n",
    "print('Datasets created')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
