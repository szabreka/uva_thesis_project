{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with this approach: The CLIP model was finetuned like a multiclass classification problem. The model presumes that each class has 1 representative in 1 batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # use CUDA device\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\") # use MacOS GPU device (e.g., for M2 chips)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # use CPU device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CLIP model - ViT B32\n",
    "model, preprocess = clip.load('ViT-B/16', device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a square-shaped image from the video (similar to 1 long image)\n",
    "#To do: what if the video has more frames than 36?\n",
    "def preprocess_video_to_image_grid_version(video_path, num_rows=6, num_cols=6):\n",
    "    #Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    #Create list for extracted frames\n",
    "    frames = []\n",
    "    #Handle if video can't be opened\n",
    "    if not video.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "    else:\n",
    "        while True:\n",
    "            is_read, frame = video.read()\n",
    "            if not is_read:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        video.release()\n",
    "    \n",
    "    # Create  and store rows in the grids\n",
    "    rows_list = []\n",
    "    for i in range(num_rows):\n",
    "        #create rows from the frames using indexes -- for example, if i=0, then between the 0th and 6th frame\n",
    "        row = np.concatenate(frames[i * num_cols: (i + 1) * num_cols], axis=1)\n",
    "        rows_list.append(row)\n",
    "    \n",
    "    # Concatenate grid vertically to create a single square-shaped image from the smoke video\n",
    "    concatenated_frames = np.concatenate(rows_list, axis=0)\n",
    "    return concatenated_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Torch Dataset class\n",
    "class ImageTitleDataset(Dataset):\n",
    "    def __init__(self, list_video_path, list_labels, class_names):\n",
    "        #Initalize image paths and corresponding texts\n",
    "        self.video_path = list_video_path\n",
    "        #Initialize labels (0 or 1)\n",
    "        self.labels = list_labels\n",
    "        #Initialize class names\n",
    "        self.class_names = class_names\n",
    "        #Transform to tensor\n",
    "        #self.transforms = ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #tranform videos into images and preprocess with clip's function\n",
    "        image = preprocess_video_to_image_grid_version(self.video_path[idx])\n",
    "        image = Image.fromarray(image)\n",
    "        image = preprocess(image)\n",
    "        #get the corresponding class names and tokenize\n",
    "        true_label = self.labels[idx]\n",
    "        label = self.class_names[true_label]\n",
    "        label = clip.tokenize(label, context_length=77, truncate=True)\n",
    "        return image, label, true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training data\n",
    "# Load the JSON metadata\n",
    "with open('data/datasets/experimental_ijmond_dataset.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "train_data = pd.DataFrame(data)\n",
    "# Prepare the list of video file paths and labels\n",
    "list_video_path = [os.path.join(\"data/ijmond_videos/\", f\"{fn}.mp4\") for fn in train_data['file_name']]\n",
    "#list_labels = dataset['label'].tolist()\n",
    "list_labels = [int(label) for label in train_data['label']]\n",
    "#Define class names in a list - it needs prompt engineering\n",
    "class_names = [\"a sequental photo of an industrial plant with clear sky above chimney, created from a video\", \"a sequental photo of an industrial plant emiting smoke from chimney, created from a video\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a sequental photo of an industrial plant emiting smoke from chimney, created from a video\n",
      "[1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]\n",
      "a sequental photo of an industrial plant emiting smoke from chimney, created from a video\n"
     ]
    }
   ],
   "source": [
    "#try labels and class names\n",
    "print(class_names[1])\n",
    "print(list_labels)\n",
    "print(class_names[list_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and data loader for training\n",
    "dataset = ImageTitleDataset(list_video_path, list_labels, class_names)\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert model's parameters to FP32 format\n",
    "#This is done so that our model loads in the provided memory\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "# Check if the device is set to CPU\n",
    "if device == \"cpu\":\n",
    "  model.float()\n",
    "\n",
    "# Prepare the optimizer - weight from other user (https://www.labellerr.com/blog/fine-tuning-clip-on-custom-dataset/)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "# Specify the loss functions - for images and for texts\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts:  tensor([[[49406,   320,  5491,  6168,  1125,   539,   550,  7520,  3912,   908,\n",
      "           1257,  6664,   633, 26821,   267,  4080,   633,   320,  1455, 49407,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  5491,  6168,  1125,   539,   550,  7520,  3912,   908,\n",
      "           1257,  6664,   633, 26821,   267,  4080,   633,   320,  1455, 49407,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  5491,  6168,  1125,   539,   550,  7520,  3912,   593,\n",
      "           3143,  2390,  4348, 26821,   267,  4080,   633,   320,  1455, 49407,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  5491,  6168,  1125,   539,   550,  7520,  3912,   908,\n",
      "           1257,  6664,   633, 26821,   267,  4080,   633,   320,  1455, 49407,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]]],\n",
      "       dtype=torch.int32)\n",
      "Used device:  cpu\n",
      "Logits_per_text after forward passing:  tensor([[25.2817, 32.5943, 26.4401, 32.3345],\n",
      "        [25.2817, 32.5943, 26.4401, 32.3345],\n",
      "        [25.9587, 32.0540, 30.4986, 32.5574],\n",
      "        [25.2817, 32.5943, 26.4401, 32.3345]], grad_fn=<TBackward0>)\n",
      "Ground truth:  tensor([1, 1, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9v/r3fdnxqn6v740_k7q9q14pym0000gn/T/ipykernel_33769/484094317.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n",
      "Epoch 0/5, Loss: 2.1630:  14%|█▍        | 1/7 [00:04<00:29,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts:  tensor([[[49406,   320,  5491,  6168,  1125,   539,   550,  7520,  3912,   593,\n",
      "           3143,  2390,  4348, 26821,   267,  4080,   633,   320,  1455, 49407,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  5491,  6168,  1125,   539,   550,  7520,  3912,   593,\n",
      "           3143,  2390,  4348, 26821,   267,  4080,   633,   320,  1455, 49407,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  5491,  6168,  1125,   539,   550,  7520,  3912,   908,\n",
      "           1257,  6664,   633, 26821,   267,  4080,   633,   320,  1455, 49407,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  5491,  6168,  1125,   539,   550,  7520,  3912,   908,\n",
      "           1257,  6664,   633, 26821,   267,  4080,   633,   320,  1455, 49407,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]]],\n",
      "       dtype=torch.int32)\n",
      "Used device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "num_epochs = 5\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "for epoch in range(num_epochs):\n",
    "  #model.train(True)\n",
    "  pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "  for batch in pbar:\n",
    "      # Zero out gradients for the optimizer (Adam)\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Extract images and texts from the batch\n",
    "      images,texts, true_label = batch \n",
    "      print('Texts: ', texts) #The texts in the batch are tokenized class names\n",
    "\n",
    "      # Print the current device (CPU or GPU)\n",
    "      print(\"Used device: \", device)\n",
    "\n",
    "      # Move images and texts to the specified device (CPU or GPU)\n",
    "      images= images.to(device)\n",
    "      texts = texts.to(device)\n",
    "\n",
    "      #Squeeze texts tensor to match the required size\n",
    "      texts = texts.squeeze(dim = 1)\n",
    "      #print(\"Shape of input tensor before forward pass: \", texts.shape)\n",
    "      #images = torch.stack([img for img in images],dim=0)\n",
    "\n",
    "      # Forward pass\n",
    "      logits_per_image, logits_per_text = model(images, texts)\n",
    "      print('Logits_per_text after forward passing: ', logits_per_text)\n",
    "\n",
    "      # Compute loss\n",
    "      ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n",
    "      #ground_truth = torch.tensor(texts[batch], dtype=torch.long, device=device)\n",
    "      #ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "      print('Ground truth: ', ground_truth)\n",
    "\n",
    "      #Transform logits to float to match required dtype\n",
    "      logits_per_image = logits_per_image.float()\n",
    "      logits_per_text = logits_per_text.float()\n",
    "\n",
    "      total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "      # Backward pass\n",
    "      total_loss.backward()\n",
    "      if device == \"cpu\":\n",
    "         optimizer.step()\n",
    "      else : \n",
    "        # Convert model's parameters to FP32 format, update, and convert back\n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "      # Update the progress bar with the current epoch and loss\n",
    "      pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to examine the dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length: 26\n",
      "Sample: 0\n",
      "Image Shape: torch.Size([3, 224, 224])\n",
      "Label: tensor([[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "         26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)\n",
      "Sample: 1\n",
      "Image Shape: torch.Size([3, 224, 224])\n",
      "Label: tensor([[49406,   320,  1125,   539,  7520,  5829,   593,   871,  6664,  4348,\n",
      "         26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)\n",
      "Sample: 2\n",
      "Image Shape: torch.Size([3, 224, 224])\n",
      "Label: tensor([[49406,   320,  1125,   539,  7520,  5829,   593,   871,  6664,  4348,\n",
      "         26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#Inspect a few examples in dataset\n",
    "\n",
    "# Create dataset\n",
    "dataset = ImageTitleDataset(list_video_path, list_labels)\n",
    "print(\"Dataset Length:\", len(dataset))\n",
    "\n",
    "# Inspect 3 samples\n",
    "for i in range(3):\n",
    "    image, label = dataset[i]\n",
    "    print(\"Sample:\", i)\n",
    "    print(\"Image Shape:\", image.shape)\n",
    "    print(\"Label:\", label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Images Shape: torch.Size([4, 3, 224, 224])\n",
      "Batch Labels: tensor([[[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   593,   871,  6664,  4348,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   593,   871,  6664,  4348,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]]],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#Inspect Batch sizes\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Iterate over a few batches\n",
    "for images, labels in dataloader:\n",
    "    print(\"Batch Images Shape:\", images.shape)\n",
    "    print(\"Batch Labels:\", labels)\n",
    "    break  # Stop after first batch\n",
    "\n",
    "# (batch_size, channel, time, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create 1 long image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create one long image from video frames\n",
    "def preprocess_video_to_image(video_path):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    #Handle if video can't be opened\n",
    "    if not video.isOpened():\n",
    "        print(\"Video file couldn't be opened\")\n",
    "    #If yes, read all video frames until the end of the video and append every frame to the frames list\n",
    "    else:\n",
    "        while True:\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        #Release video\n",
    "        video.release()\n",
    "    #Concetanate the frames in the list together\n",
    "    concatenated_frames = np.concatenate(frames, axis=1)\n",
    "    return concatenated_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to save model and open the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to save the trained model\n",
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"model_checkpoint/model_clip_1.pt\") #just change to your preferred folder/filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to load the saved model :\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "checkpoint = torch.load(\"model_checkpoint/model_clip_1.pt\")\n",
    "\n",
    "# Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
