{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with this approach: The CLIP model was finetuned like a multiclass classification problem. The model presumes that each class has 1 representative in 1 batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <85A36C65-3F71-3C3B-B529-961AE17DBE73> /Users/szaboreka/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <F8622D92-25A9-3A61-A089-C917FDA36C1B> /Users/szaboreka/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # use CUDA device\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\") # use MacOS GPU device (e.g., for M2 chips)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # use CPU device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CLIP model - ViT B32\n",
    "model, preprocess = clip.load('ViT-B/32', device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a square-shaped image from the video (similar to 1 long image)\n",
    "#To do: what if the video has more frames than 36?\n",
    "def preprocess_video_to_image_grid_version(video_path, num_rows=6, num_cols=6):\n",
    "    #Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    #Create list for extracted frames\n",
    "    frames = []\n",
    "    #Handle if video can't be opened\n",
    "    if not video.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "    else:\n",
    "        while True:\n",
    "            is_read, frame = video.read()\n",
    "            if not is_read:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        video.release()\n",
    "    \n",
    "    # Create  and store rows in the grids\n",
    "    rows_list = []\n",
    "    for i in range(num_rows):\n",
    "        #create rows from the frames using indexes -- for example, if i=0, then between the 0th and 6th frame\n",
    "        row = np.concatenate(frames[i * num_cols: (i + 1) * num_cols], axis=1)\n",
    "        rows_list.append(row)\n",
    "    \n",
    "    # Concatenate grid vertically to create a single square-shaped image from the smoke video\n",
    "    concatenated_frames = np.concatenate(rows_list, axis=0)\n",
    "    return concatenated_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Torch Dataset class\n",
    "class ImageTitleDataset(Dataset):\n",
    "    def __init__(self, list_video_path, list_labels, class_names):\n",
    "        #Initalize image paths and corresponding texts\n",
    "        self.video_path = list_video_path\n",
    "        #Initialize labels (0 or 1)\n",
    "        self.labels = list_labels\n",
    "        #Initialize class names\n",
    "        self.class_names = class_names\n",
    "        #Transform to tensor\n",
    "        #self.transforms = ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #tranform videos into images and preprocess with clip's function\n",
    "        image = preprocess_video_to_image_grid_version(self.video_path[idx])\n",
    "        image = Image.fromarray(image)\n",
    "        image = preprocess(image)\n",
    "        #get the corresponding class names and tokenize\n",
    "        true_label = self.labels[idx]\n",
    "        label = self.class_names[true_label]\n",
    "        label = clip.tokenize(label, context_length=77, truncate=True)\n",
    "        return image, label, true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training data\n",
    "# Load the JSON metadata\n",
    "with open('data/datasets/experimental_ijmond_dataset.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "train_data = pd.DataFrame(data)\n",
    "# Prepare the list of video file paths and labels\n",
    "list_video_path = [os.path.join(\"data/ijmond_videos/\", f\"{fn}.mp4\") for fn in train_data['file_name']]\n",
    "#list_labels = dataset['label'].tolist()\n",
    "list_labels = [int(label) for label in train_data['label']]\n",
    "#Define class names in a list - it needs prompt engineering\n",
    "class_names = [\"a photo of industrial plants with clear sky above chimney\", \"a photo of industrial plants emiting smoke from chimney\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photo of industrial plants emiting smoke from chimney\n",
      "[1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]\n",
      "a photo of industrial plants emiting smoke from chimney\n"
     ]
    }
   ],
   "source": [
    "#try labels and class names\n",
    "print(class_names[1])\n",
    "print(list_labels)\n",
    "print(class_names[list_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and data loader for training\n",
    "dataset = ImageTitleDataset(list_video_path, list_labels, class_names)\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert model's parameters to FP32 format\n",
    "#This is done so that our model loads in the provided memory\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "# Check if the device is set to CPU\n",
    "if device == \"cpu\":\n",
    "  model.float()\n",
    "\n",
    "# Prepare the optimizer - weight from other user (https://www.labellerr.com/blog/fine-tuning-clip-on-custom-dataset/)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "# Specify the loss functions - for images and for texts\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts:  tensor([[[49406,   320,  1125,   539,  7520,  5829,   593,  3143,  2390,  4348,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   593,  3143,  2390,  4348,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]]],\n",
      "       dtype=torch.int32)\n",
      "Used device:  cpu\n",
      "Logits_per_text after forward passing:  tensor([[22.3134, 22.2224, 20.9886, 23.0032],\n",
      "        [20.3260, 19.7557, 19.1832, 21.5470],\n",
      "        [20.3260, 19.7557, 19.1832, 21.5470],\n",
      "        [22.3134, 22.2224, 20.9886, 23.0032]], grad_fn=<TBackward0>)\n",
      "Ground truth:  tensor([0, 1, 1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9v/r3fdnxqn6v740_k7q9q14pym0000gn/T/ipykernel_76775/2310193699.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n",
      "Epoch 0/5, Loss: 1.8675:  14%|█▍        | 1/7 [00:03<00:20,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts:  tensor([[[49406,   320,  1125,   539,  7520,  5829,   593,  3143,  2390,  4348,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   593,  3143,  2390,  4348,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]]],\n",
      "       dtype=torch.int32)\n",
      "Used device:  cpu\n",
      "Logits_per_text after forward passing:  tensor([[ 5.9609,  5.6055,  2.5820,  3.1875],\n",
      "        [16.8594, 17.3125, 14.8906, 15.8516],\n",
      "        [16.8594, 17.3125, 14.8906, 15.8516],\n",
      "        [ 5.9609,  5.6055,  2.5820,  3.1875]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "Ground truth:  tensor([0, 1, 1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, Loss: 3.6063:  29%|██▊       | 2/7 [01:46<05:11, 62.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts:  tensor([[[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   593,  3143,  2390,  4348,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   593,  3143,  2390,  4348,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]]],\n",
      "       dtype=torch.int32)\n",
      "Used device:  cpu\n",
      "Logits_per_text after forward passing:  tensor([[ 8.8516, 11.9062,  4.2852, 12.3281],\n",
      "        [ 8.8516, 11.9062,  4.2852, 12.3281],\n",
      "        [13.0938, 15.9375,  8.3125, 16.5156],\n",
      "        [13.0938, 15.9375,  8.3125, 16.5156]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "Ground truth:  tensor([1, 1, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, Loss: 3.6239:  43%|████▎     | 3/7 [03:25<05:14, 78.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts:  tensor([[[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]]],\n",
      "       dtype=torch.int32)\n",
      "Used device:  cpu\n",
      "Logits_per_text after forward passing:  tensor([[ 8.2188, 16.1719, 19.4688,  8.4844],\n",
      "        [ 8.2188, 16.1719, 19.4688,  8.4844],\n",
      "        [ 8.2188, 16.1719, 19.4688,  8.4844],\n",
      "        [ 8.2188, 16.1719, 19.4688,  8.4844]], dtype=torch.float16,\n",
      "       grad_fn=<TBackward0>)\n",
      "Ground truth:  tensor([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "  #model.train(True)\n",
    "  pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "  for batch in pbar:\n",
    "      # Zero out gradients for the optimizer (Adam)\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Extract images and texts from the batch\n",
    "      images,texts, true_label = batch \n",
    "      print('Texts: ', texts) #The texts in the batch are tokenized class names\n",
    "\n",
    "      # Print the current device (CPU or GPU)\n",
    "      print(\"Used device: \", device)\n",
    "\n",
    "      # Move images and texts to the specified device (CPU or GPU)\n",
    "      images= images.to(device)\n",
    "      texts = texts.to(device)\n",
    "\n",
    "      #Squeeze texts tensor to match the required size\n",
    "      texts = texts.squeeze(dim = 1)\n",
    "      #print(\"Shape of input tensor before forward pass: \", texts.shape)\n",
    "      #images = torch.stack([img for img in images],dim=0)\n",
    "\n",
    "      # Forward pass\n",
    "      logits_per_image, logits_per_text = model(images, texts)\n",
    "      print('Logits_per_text after forward passing: ', logits_per_text)\n",
    "\n",
    "      # Compute loss\n",
    "      ground_truth = torch.tensor(true_label, dtype=torch.long, device=device)\n",
    "      #ground_truth = torch.tensor(texts[batch], dtype=torch.long, device=device)\n",
    "      #ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "      print('Ground truth: ', ground_truth)\n",
    "\n",
    "      #Transform logits to flote to match required dtype\n",
    "      logits_per_image = logits_per_image.float()\n",
    "      logits_per_text = logits_per_text.float()\n",
    "\n",
    "      total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "      \n",
    "      # Backward pass\n",
    "      total_loss.backward()\n",
    "      if device == \"cpu\":\n",
    "         optimizer.step()\n",
    "      else : \n",
    "        # Convert model's parameters to FP32 format, update, and convert back\n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "      # Update the progress bar with the current epoch and loss\n",
    "      pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to examine the dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length: 26\n",
      "Sample: 0\n",
      "Image Shape: torch.Size([3, 224, 224])\n",
      "Label: tensor([[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "         26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)\n",
      "Sample: 1\n",
      "Image Shape: torch.Size([3, 224, 224])\n",
      "Label: tensor([[49406,   320,  1125,   539,  7520,  5829,   593,   871,  6664,  4348,\n",
      "         26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)\n",
      "Sample: 2\n",
      "Image Shape: torch.Size([3, 224, 224])\n",
      "Label: tensor([[49406,   320,  1125,   539,  7520,  5829,   593,   871,  6664,  4348,\n",
      "         26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#Inspect a few examples in dataset\n",
    "\n",
    "# Create dataset\n",
    "dataset = ImageTitleDataset(list_video_path, list_labels)\n",
    "print(\"Dataset Length:\", len(dataset))\n",
    "\n",
    "# Inspect 3 samples\n",
    "for i in range(3):\n",
    "    image, label = dataset[i]\n",
    "    print(\"Sample:\", i)\n",
    "    print(\"Image Shape:\", image.shape)\n",
    "    print(\"Label:\", label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Images Shape: torch.Size([4, 3, 224, 224])\n",
      "Batch Labels: tensor([[[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   593,   871,  6664,  4348,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   593,   871,  6664,  4348,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]],\n",
      "\n",
      "        [[49406,   320,  1125,   539,  7520,  5829,   908,  1257,  6664,   633,\n",
      "          26821, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0]]],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#Inspect Batch sizes\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Iterate over a few batches\n",
    "for images, labels in dataloader:\n",
    "    print(\"Batch Images Shape:\", images.shape)\n",
    "    print(\"Batch Labels:\", labels)\n",
    "    break  # Stop after first batch\n",
    "\n",
    "# (batch_size, channel, time, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to examine CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=warn)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x13fe39e40>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code to examine the preprocess function\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter positional_embedding, shape torch.Size([77, 512])\n",
      "Parameter text_projection, shape torch.Size([512, 512])\n",
      "Parameter logit_scale, shape torch.Size([])\n",
      "Parameter visual.class_embedding, shape torch.Size([768])\n",
      "Parameter visual.positional_embedding, shape torch.Size([50, 768])\n",
      "Parameter visual.proj, shape torch.Size([768, 512])\n",
      "Parameter visual.conv1.weight, shape torch.Size([768, 3, 32, 32])\n",
      "Parameter visual.ln_pre.weight, shape torch.Size([768])\n",
      "Parameter visual.ln_pre.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.0.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.0.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.0.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.0.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.0.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.0.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.0.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.0.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.0.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.0.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.0.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.0.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.1.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.1.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.1.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.1.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.1.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.1.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.1.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.1.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.1.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.1.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.1.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.1.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.2.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.2.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.2.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.2.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.2.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.2.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.2.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.2.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.2.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.2.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.2.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.2.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.3.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.3.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.3.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.3.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.3.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.3.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.3.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.3.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.3.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.3.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.3.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.3.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.4.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.4.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.4.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.4.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.4.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.4.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.4.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.4.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.4.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.4.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.4.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.4.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.5.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.5.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.5.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.5.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.5.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.5.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.5.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.5.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.5.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.5.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.5.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.5.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.6.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.6.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.6.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.6.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.6.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.6.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.6.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.6.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.6.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.6.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.6.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.6.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.7.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.7.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.7.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.7.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.7.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.7.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.7.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.7.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.7.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.7.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.7.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.7.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.8.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.8.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.8.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.8.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.8.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.8.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.8.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.8.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.8.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.8.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.8.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.8.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.9.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.9.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.9.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.9.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.9.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.9.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.9.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.9.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.9.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.9.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.9.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.9.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.10.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.10.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.10.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.10.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.10.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.10.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.10.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.10.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.10.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.10.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.10.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.10.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.11.attn.in_proj_weight, shape torch.Size([2304, 768])\n",
      "Parameter visual.transformer.resblocks.11.attn.in_proj_bias, shape torch.Size([2304])\n",
      "Parameter visual.transformer.resblocks.11.attn.out_proj.weight, shape torch.Size([768, 768])\n",
      "Parameter visual.transformer.resblocks.11.attn.out_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.11.ln_1.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.11.ln_1.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.11.mlp.c_fc.weight, shape torch.Size([3072, 768])\n",
      "Parameter visual.transformer.resblocks.11.mlp.c_fc.bias, shape torch.Size([3072])\n",
      "Parameter visual.transformer.resblocks.11.mlp.c_proj.weight, shape torch.Size([768, 3072])\n",
      "Parameter visual.transformer.resblocks.11.mlp.c_proj.bias, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.11.ln_2.weight, shape torch.Size([768])\n",
      "Parameter visual.transformer.resblocks.11.ln_2.bias, shape torch.Size([768])\n",
      "Parameter visual.ln_post.weight, shape torch.Size([768])\n",
      "Parameter visual.ln_post.bias, shape torch.Size([768])\n",
      "Parameter transformer.resblocks.0.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.0.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.0.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.0.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.0.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.0.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.0.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.0.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.0.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.0.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.0.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.0.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.1.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.1.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.1.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.1.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.1.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.1.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.1.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.1.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.1.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.1.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.1.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.1.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.2.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.2.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.2.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.2.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.2.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.2.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.2.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.2.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.2.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.2.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.2.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.2.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.3.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.3.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.3.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.3.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.3.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.3.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.3.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.3.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.3.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.3.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.3.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.3.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.4.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.4.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.4.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.4.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.4.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.4.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.4.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.4.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.4.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.4.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.4.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.4.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.5.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.5.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.5.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.5.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.5.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.5.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.5.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.5.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.5.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.5.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.5.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.5.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.6.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.6.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.6.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.6.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.6.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.6.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.6.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.6.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.6.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.6.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.6.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.6.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.7.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.7.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.7.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.7.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.7.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.7.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.7.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.7.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.7.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.7.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.7.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.7.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.8.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.8.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.8.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.8.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.8.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.8.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.8.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.8.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.8.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.8.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.8.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.8.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.9.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.9.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.9.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.9.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.9.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.9.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.9.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.9.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.9.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.9.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.9.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.9.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.10.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.10.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.10.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.10.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.10.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.10.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.10.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.10.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.10.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.10.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.10.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.10.ln_2.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.11.attn.in_proj_weight, shape torch.Size([1536, 512])\n",
      "Parameter transformer.resblocks.11.attn.in_proj_bias, shape torch.Size([1536])\n",
      "Parameter transformer.resblocks.11.attn.out_proj.weight, shape torch.Size([512, 512])\n",
      "Parameter transformer.resblocks.11.attn.out_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.11.ln_1.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.11.ln_1.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.11.mlp.c_fc.weight, shape torch.Size([2048, 512])\n",
      "Parameter transformer.resblocks.11.mlp.c_fc.bias, shape torch.Size([2048])\n",
      "Parameter transformer.resblocks.11.mlp.c_proj.weight, shape torch.Size([512, 2048])\n",
      "Parameter transformer.resblocks.11.mlp.c_proj.bias, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.11.ln_2.weight, shape torch.Size([512])\n",
      "Parameter transformer.resblocks.11.ln_2.bias, shape torch.Size([512])\n",
      "Parameter token_embedding.weight, shape torch.Size([49408, 512])\n",
      "Parameter ln_final.weight, shape torch.Size([512])\n",
      "Parameter ln_final.bias, shape torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter {name}, shape {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create 1 long image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create one long image from video frames\n",
    "def preprocess_video_to_image(video_path):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    #Handle if video can't be opened\n",
    "    if not video.isOpened():\n",
    "        print(\"Video file couldn't be opened\")\n",
    "    #If yes, read all video frames until the end of the video and append every frame to the frames list\n",
    "    else:\n",
    "        while True:\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        #Release video\n",
    "        video.release()\n",
    "    #Concetanate the frames in the list together\n",
    "    concatenated_frames = np.concatenate(frames, axis=1)\n",
    "    return concatenated_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to save model and open the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to save the trained model\n",
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"model_checkpoint/model_clip_1.pt\") #just change to your preferred folder/filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to load the saved model :\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "checkpoint = torch.load(\"model_checkpoint/model_clip_1.pt\")\n",
    "\n",
    "# Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
