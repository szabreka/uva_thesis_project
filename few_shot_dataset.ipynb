{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset for downloaded videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import os\n",
    "from EDA.eda_functions import get_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ijmond_df = pd.read_json('data/datasets/ijmond_dataset.json')\n",
    "#predict label for each video in ijmond dir\n",
    "files = os.listdir(\"data/ijmond_videos/\")\n",
    "\n",
    "# Extract file names without extensions\n",
    "file_names = [os.path.splitext(file)[0] for file in files]\n",
    "\n",
    "# Filter the ijmond DataFrame to include only rows with matching file names\n",
    "filtered_ijmond_df = ijmond_df[ijmond_df['file_name'].isin(file_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9v/r3fdnxqn6v740_k7q9q14pym0000gn/T/ipykernel_58501/2910654267.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_ijmond_df['label'] = filtered_ijmond_df.apply(get_label, axis = 1)\n"
     ]
    }
   ],
   "source": [
    "filtered_ijmond_df['label'] = filtered_ijmond_df.apply(get_label, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    13\n",
       "0    13\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ijmond_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the created dataset\n",
    "filtered_ijmond_df.to_json(\"data/datasets/experimental_ijmond_dataset.json\", orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create scrpit that can transform videos to images and use clip on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 92.43%\n",
      "a photo of industrial plants emiting smoke from chimney: 7.59%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 91.99%\n",
      "a photo of industrial plants emiting smoke from chimney: 8.04%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 92.33%\n",
      "a photo of industrial plants emiting smoke from chimney: 7.70%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 85.60%\n",
      "a photo of industrial plants emiting smoke from chimney: 14.42%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 80.57%\n",
      "a photo of industrial plants emiting smoke from chimney: 19.43%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 87.35%\n",
      "a photo of industrial plants emiting smoke from chimney: 12.59%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 70.21%\n",
      "a photo of industrial plants emiting smoke from chimney: 29.74%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 92.82%\n",
      "a photo of industrial plants emiting smoke from chimney: 7.16%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 95.70%\n",
      "a photo of industrial plants emiting smoke from chimney: 4.34%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 85.99%\n",
      "a photo of industrial plants emiting smoke from chimney: 14.04%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 90.28%\n",
      "a photo of industrial plants emiting smoke from chimney: 9.67%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 94.92%\n",
      "a photo of industrial plants emiting smoke from chimney: 5.11%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 96.04%\n",
      "a photo of industrial plants emiting smoke from chimney: 3.90%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 89.36%\n",
      "a photo of industrial plants emiting smoke from chimney: 10.67%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 93.36%\n",
      "a photo of industrial plants emiting smoke from chimney: 6.66%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 93.41%\n",
      "a photo of industrial plants emiting smoke from chimney: 6.56%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 69.63%\n",
      "a photo of industrial plants emiting smoke from chimney: 30.42%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 91.99%\n",
      "a photo of industrial plants emiting smoke from chimney: 8.04%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 82.67%\n",
      "a photo of industrial plants emiting smoke from chimney: 17.32%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 85.40%\n",
      "a photo of industrial plants emiting smoke from chimney: 14.61%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 81.25%\n",
      "a photo of industrial plants emiting smoke from chimney: 18.71%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 95.90%\n",
      "a photo of industrial plants emiting smoke from chimney: 4.15%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 95.61%\n",
      "a photo of industrial plants emiting smoke from chimney: 4.40%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 87.06%\n",
      "a photo of industrial plants emiting smoke from chimney: 12.94%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 79.05%\n",
      "a photo of industrial plants emiting smoke from chimney: 20.95%\n",
      "\n",
      "Predictions for the entire video:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 89.21%\n",
      "a photo of industrial plants emiting smoke from chimney: 10.82%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from EDA.eda_functions import get_label\n",
    "\n",
    "# Define device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # use CUDA device\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") # use MacOS GPU device (e.g., for M2 chips)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # use CPU device\n",
    "device\n",
    "\n",
    "#Load the model\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "#Define class names in a list - it needs prompt engineering\n",
    "class_names = [\"a photo of industrial plants emiting smoke from chimney\", \"a photo of industrial plants with no smoke above chimney\"]\n",
    "\n",
    "#Function to create an image from the video\n",
    "def preprocess_video_to_image(video_path):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    if not video.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "    else:\n",
    "        while True:\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        video.release()\n",
    "    # Concatenate frames horizontally to create a single image\n",
    "    concatenated_frame = np.concatenate(frames, axis=1)\n",
    "    return concatenated_frame\n",
    "\n",
    "#func to get the true label of the video\n",
    "def get_true_label(file_name):\n",
    "    row = ijmond_df[ijmond_df['file_name'] == file_name].iloc[0]\n",
    "    return get_label(row)\n",
    "\n",
    "#function to gwt label of the image produced from the video\n",
    "def vanilla_clip(video_path):\n",
    "    # Preprocess video into a single image\n",
    "    video_image = preprocess_video_to_image(video_path)\n",
    "    \n",
    "    # Read image and preprocess\n",
    "    image = preprocess(Image.fromarray(video_image)).unsqueeze(0).to(device)\n",
    "\n",
    "    # Prepare text inputs based on class names list\n",
    "    text_inputs = clip.tokenize(class_names).to(device)\n",
    "\n",
    "    # Calculate features\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    # Calculate similarity\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    #values are the probabilities, indices are the classes\n",
    "    values, indices = similarity[0].topk(2)\n",
    "\n",
    "    # Print predictions\n",
    "    print(f\"\\nPredictions for the entire video:\\n\")\n",
    "    for value, index in zip(values, indices):\n",
    "        print(f\"{class_names[index]:>16s}: {100 * value.item():.2f}%\")\n",
    "\n",
    "#predict label for each video in ijmond dir\n",
    "files = os.listdir(\"data/ijmond_videos/\")\n",
    "for file in files:\n",
    "    video_path = f\"data/ijmond_videos/{file}\"\n",
    "    file_name = file.split('.')[0]\n",
    "    vanilla_clip(video_path)\n",
    "    get_true_label(file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for the entire video data/ijmond_videos/5PurGkmy0aw-1.mp4:\n",
      "\n",
      "a photo of industrial plants with no smoke above chimney: 95.90%\n",
      "a photo of industrial plants emiting smoke from chimney: 4.15%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from EDA.eda_functions import get_label\n",
    "\n",
    "# Define device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # use CUDA device\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") # use MacOS GPU device (e.g., for M2 chips)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # use CPU device\n",
    "device\n",
    "\n",
    "#Load the model\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "#Define class names in a list - it needs prompt engineering\n",
    "class_names = [\"a photo of industrial plants emiting smoke from chimney\", \"a photo of industrial plants with no smoke above chimney\"]\n",
    "\n",
    "#Function to create an image from the video\n",
    "def preprocess_video_to_image_grid_version(video_path, num_rows=6, num_cols=6):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    if not video.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "    else:\n",
    "        while True:\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        video.release()\n",
    "    \n",
    "        # Split frames into grid\n",
    "    grid = []\n",
    "    for i in range(num_rows):\n",
    "        row = np.concatenate(frames[i * num_cols: (i + 1) * num_cols], axis=1)\n",
    "        grid.append(row)\n",
    "    \n",
    "    # Concatenate grid vertically to create a single image\n",
    "    concatenated_frame = np.concatenate(grid, axis=0)\n",
    "    return concatenated_frame\n",
    "\n",
    "#func to get the true label of the video\n",
    "def get_true_label(file_name):\n",
    "    row = ijmond_df[ijmond_df['file_name'] == file_name].iloc[0]\n",
    "    return get_label(row)\n",
    "\n",
    "#function to gwt label of the image produced from the video\n",
    "def vanilla_clip(video_path):\n",
    "    # Preprocess video into a single image\n",
    "    video_image = preprocess_video_to_image_grid_version(video_path)\n",
    "    # Convert numpy array to PIL Image\n",
    "    #image = Image.fromarray(video_image)\n",
    "    # Save the image\n",
    "    #image.save('square_image.png')\n",
    "    \n",
    "    # Read image and preprocess\n",
    "    image = preprocess(Image.fromarray(video_image)).unsqueeze(0).to(device)\n",
    "\n",
    "    # Prepare text inputs based on class names list\n",
    "    text_inputs = clip.tokenize(class_names).to(device)\n",
    "\n",
    "    # Calculate features\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    # Calculate similarity\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    #values are the probabilities, indices are the classes\n",
    "    values, indices = similarity[0].topk(2)\n",
    "\n",
    "    # Print predictions\n",
    "    print(f\"\\nPredictions for the entire video {video_path}:\\n\")\n",
    "    for value, index in zip(values, indices):\n",
    "        print(f\"{class_names[index]:>16s}: {100 * value.item():.2f}%\")\n",
    "\n",
    "#predict label for each video in ijmond dir\n",
    "files = os.listdir(\"data/ijmond_videos/\")\n",
    "for file in files:\n",
    "    video_path = f\"data/ijmond_videos/{file}\"\n",
    "    file_name = file.split('.')[0]\n",
    "    vanilla_clip(video_path)\n",
    "    get_true_label(file_name)\n",
    "\n",
    "#test:\n",
    "#vanilla_clip('data/ijmond_videos/5PurGkmy0aw-1.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 89\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(all_features)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), torch\u001b[38;5;241m.\u001b[39mcat(all_labels)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Calculate the video features\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m train_features, train_labels \u001b[38;5;241m=\u001b[39m get_features(train_dataset)\n\u001b[1;32m     90\u001b[0m test_features, test_labels \u001b[38;5;241m=\u001b[39m get_features(test_dataset)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Perform logistic regression\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 80\u001b[0m, in \u001b[0;36mget_features\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     77\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m videos, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m     81\u001b[0m         features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(videos\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     83\u001b[0m         all_features\u001b[38;5;241m.\u001b[39mappend(features)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[6], line 61\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     59\u001b[0m video_frames \u001b[38;5;241m=\u001b[39m preprocess_video_to_image(video_path)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 61\u001b[0m     video_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(video_frames)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m video_frames, label\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mresize(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantialias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:476\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    472\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    474\u001b[0m         )\n\u001b[0;32m--> 476\u001b[0m _, image_height, image_width \u001b[38;5;241m=\u001b[39m get_dimensions(img)\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    478\u001b[0m     size \u001b[38;5;241m=\u001b[39m [size]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:78\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/_functional_pil.py:31\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     29\u001b[0m     width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [channels, height, width]\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "# Define device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # use CUDA device\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") # use MacOS GPU device (e.g., for M2 chips)\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # use CPU device\n",
    "device\n",
    "\n",
    "#Load the model\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "ijmond_df = pd.read_json('data/datasets/ijmond_dataset.json')\n",
    "\n",
    "#Define class names in a list - it needs prompt engineering\n",
    "class_names = [\"a photo of industrial plants emiting smoke from chimney\", \"a photo of industrial plants with no smoke above chimney\"]\n",
    "\n",
    "def preprocess_video_to_image(video_path):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    if not video.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "    else:\n",
    "        while True:\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        video.release()\n",
    "    # Concatenate frames horizontally to create a single image\n",
    "    concatenated_frame = np.concatenate(frames, axis=1)\n",
    "    return concatenated_frame\n",
    "\n",
    "# Define the dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.data[idx]['file_name']\n",
    "        label = int(self.data[idx]['label'])  # Convert label to integer\n",
    "        video_path = os.path.join('data/ijmond_videos', f\"{file_name}.mp4\")\n",
    "        video_frames = preprocess_video_to_image(video_path)\n",
    "        if self.transform:\n",
    "            video_frames = self.transform(video_frames)\n",
    "        return video_frames, label\n",
    "\n",
    "# Load the dataset\n",
    "with open('data/datasets/experimental_ijmond_dataset.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = MyDataset(train_data, transform=preprocess)\n",
    "test_dataset = MyDataset(test_data, transform=preprocess)\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for videos, labels in tqdm(DataLoader(dataset, batch_size=1)):\n",
    "            features = model.encode_image(videos.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the video features\n",
    "train_features, train_labels = get_features(train_dataset)\n",
    "test_features, test_labels = get_features(test_dataset)\n",
    "\n",
    "# Perform logistic regression\n",
    "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features)\n",
    "accuracy = np.mean((test_labels == predictions).astype(float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 427,616,513\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, preprocess = clip.load(\"ViT-L/14\")\n",
    "model.eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 335M/335M [00:28<00:00, 12.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, preprocess = clip.load(\"ViT-B/16\")\n",
    "model.eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
